{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MangoGrove/MangoGrove.github.io/blob/main/HWK02/CSE_40657_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP HW2: Parsing\n",
        "*Last Updated: Sep 25 3:28 PM*\n",
        "\n",
        "For **Google Colab**: Runtime > Change runtime type > Hardware accelerator > GPU"
      ],
      "metadata": {
        "id": "H9UVrLjSDK3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Brisny Rodriguez Flores"
      ],
      "metadata": {
        "id": "Vg7hhEUUjrs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jdc\n",
        "import jdc"
      ],
      "metadata": {
        "id": "nysaMbFqEIk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "370a5e5d-2436-42b7-fe21-1de5099ec579"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jdc in /usr/local/lib/python3.12/dist-packages (0.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://raw.githubusercontent.com/aarsri/nlp_hw2/refs/heads/main/utils.py\n",
        "!wget -nc https://raw.githubusercontent.com/aarsri/nlp_hw2/refs/heads/main/trees.py"
      ],
      "metadata": {
        "id": "jVAS8M5CKkc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea35bb28-7a45-4130-b548-70e6c6093fa9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-03 15:41:12--  https://raw.githubusercontent.com/aarsri/nlp_hw2/refs/heads/main/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 975 [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]     975  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-03 15:41:13 (32.8 MB/s) - ‘utils.py’ saved [975/975]\n",
            "\n",
            "--2025-10-03 15:41:13--  https://raw.githubusercontent.com/aarsri/nlp_hw2/refs/heads/main/trees.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9860 (9.6K) [text/plain]\n",
            "Saving to: ‘trees.py’\n",
            "\n",
            "trees.py            100%[===================>]   9.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-03 15:41:13 (64.5 MB/s) - ‘trees.py’ saved [9860/9860]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aarsri/nlp_hw2.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTTPDKFelEnO",
        "outputId": "27ff168a-be1c-4afb-96c3-ea4d3edaad78"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp_hw2'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 62 (delta 28), reused 12 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (62/62), 55.67 KiB | 904.00 KiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Bi-LSTM POS Tagger\n",
        "In this part, you will build a part-of-speech tagger using the bidirectional LSTM architecture."
      ],
      "metadata": {
        "id": "XTybDZltGwxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import Vocab, read_pos_file"
      ],
      "metadata": {
        "id": "1gdSjFqmj1-u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "nltk.download('treebank')\n",
        "brown = list(treebank.tagged_sents())"
      ],
      "metadata": {
        "id": "j0OaEhyyDN6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4089897-1ddf-4851-e656-d078e689195f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[4 points]** Implement a **bidirectional** LSTM with embedding size 128 and hidden size 256 using torch.nn.LSTM with appropriate parameters set. Remember that PyTorch's cross entropy loss includes softmax."
      ],
      "metadata": {
        "id": "gKFCXjXIHFHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMTagger(nn.Module):\n",
        "\tdef __init__(self, data, embedding_dim, hidden_dim):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.words = Vocab()\n",
        "\t\tself.tags = Vocab()\n",
        "\n",
        "\n",
        "\t\tprint(self.words)\n",
        "\t\tself.words.add('<UNK>')\n",
        "\t\tself.tags.add('<UNK>')\n",
        "\n",
        "\n",
        "\t\tfor sent in data:\n",
        "\t\t\tfor word, tag in sent:\n",
        "\t\t\t\tself.words.add(word)\n",
        "\t\t\t\tself.tags.add(tag)\n",
        "\n",
        "\t\t\"\"\"TODO: Populate self.words and self.tags as two vocabulary objects using the Vocab class in utils.py.\n",
        "\t\tThis will allow you to easily numberize and denumberize the word vocabulary as well as the tagset.\n",
        "\t\tMake sure to add <UNK> self.words.\"\"\"\n",
        "\t\tself.embedding_dim = embedding_dim\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\t\t\"\"\"\tTODO: Initialize layers.\"\"\"\n",
        "\t\t# embedding\n",
        "\t\tself.embedding = nn.Embedding(len(self.words), embedding_dim)\n",
        "\t\t# lstm\n",
        "\t\tself.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "\t\t# dropout\n",
        "\t\tself.dropout = nn.Dropout(0.2)\n",
        "\t\t# W_out\n",
        "\t\tself.W_out = nn.Linear(hidden_dim * 2, len(self.tags))\n",
        "\t\t#raise NotImplementedError"
      ],
      "metadata": {
        "id": "RZLxOmiHDkj0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%add_to BiLSTMTagger\n",
        "def forward(self, sentence):\n",
        "\t\"\"\"\tTODO: Pass the sentence through the layers of the model.\n",
        "\t\t* Because we are using the built-in LSTM, we can pass in an entire sentence rather than iterating through the tokens.\n",
        "\t\t* IMPORTANT: Because we are dealing with a full sentence now, we have to do minor reshaping.\n",
        "\t\t\t* Before passing the embeddings into the LSTM, we have to do `embeddings.view(len(sentence), 1, -1)`\n",
        "\t\t\t* Before passing the LSTM output into dropout, we have to do `lstm_out.view(len(sentence), -1)`\n",
        "\t\t* Return the output scores from the model (pre-softmax). This will be of shape: len(sentence) x total number of tags, meaning each row corresponds to a word, and the values in each row are the scores for all possible POS tags for that word.\"\"\"\n",
        "\tembedding = self.embedding(sentence)\n",
        "\tembedding = embedding.view(len(sentence), 1, -1) #before sending off the embeddings\n",
        "\tlstm_out, _ = self.lstm(embedding.view(len(sentence), 1, -1))\n",
        "\tlstm_out = lstm_out.view(len(sentence), -1) #before sending off the lstm_output\n",
        "\tdrop_out = self.dropout(lstm_out.view(len(sentence), -1))\n",
        "\tscores = self.W_out(drop_out) #pre soft max\n",
        "\treturn scores\n",
        "\n",
        "\t#raise NotImplementedError"
      ],
      "metadata": {
        "id": "m4_k6KXqERUf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%add_to BiLSTMTagger\n",
        "def predict(self, scores):\n",
        "\t\"\"\"\tTODO: Return the most likely tag sequence.\n",
        "\t\t* When the dim argument is provided, torch.argmax(input, dim) returns a tensor containing the indices of the maximum values along that specified dimension.\n",
        "\t\t* Since each row of scores corresponds to a different word, and each column corresponds to a different tag, specificy dim=1 (take max along columns).\"\"\"\n",
        "\n",
        "\treturn torch.argmax(scores, dim=1)\n",
        "\n",
        "\t#raise NotImplementedError"
      ],
      "metadata": {
        "id": "lDuGWc36E0FL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[3 points]** Write **training** and **evaluation** procedures. These should be similar to HW1 Part 2 and 3.\n",
        "- For debugging purposes only, shorten the training data by commenting out the train_sents += brown line. Accuracy will be high because the train set is otherwise a combination of ATIS and BROWN, while the test set is ATIS. Commenting out this line will still have the model trained and tested on ATIS, yielding a high accuracy.  However, it will not be generalized well enough to do the free response.\n"
      ],
      "metadata": {
        "id": "hQ9dWqMWHONW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%add_to BiLSTMTagger\n",
        "\"\"\"\tTODO: This function is very similar to fit() from HW1.\"\"\"\n",
        "def fit(self, data, lr=0.01, epochs=5):\n",
        "\t# 1. Initialize the optimizer. Use `torch.optim.Adam` with `self.parameters()` and `lr`.\n",
        "\toptimizer = torch.optim.Adam(self.parameters(), lr)\n",
        "\t# 2. Set a loss function variable to `nn.CrossEntropyLoss()`. It includes softmax.\n",
        "\tloss_function = nn.CrossEntropyLoss()\n",
        "\t# 3. Loop through the specified number of epochs.\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tcurr_time = time.time()\n",
        "\t\t#\t 1. Put the model into training mode using `self.train()`.\n",
        "\t\tself.train()\n",
        "\t\t#\t 2. Shuffle the training data using random.shuffle().\n",
        "\t\trandom.shuffle(data)\n",
        "\t\t#\t 3. Initialize variables to keep track of the total loss (`total_loss`) and the total number of tokens (`total_tokens`).\n",
        "\t\ttotal_loss = 0\n",
        "\t\ttotal_tokens = 0\n",
        "\t\t#\t 4. Loop over each sentence in the training data.\n",
        "\t\tfor sentence in data:\n",
        "\t\t\t#1. Produce a numberized sequence of the words in the sentence. Make words lowercase first, and convert the sequence to a tensor using something like: `torch.tensor(idxs, dtype=torch.long)`.\n",
        "\t\t\t#sentence.lower()\n",
        "\t\t\t#idxs = [self.words.numberize(word.lower) for word in sentence]\n",
        "\t\t\t#print(sentence)\n",
        "\t\t\tpalabras = (word.lower() for word, tag in sentence)\n",
        "\t\t\tidxs = [self.words.numberize(word) for word in palabras]\n",
        "\n",
        "\t\t\t#print(\"hello\")\n",
        "\n",
        "\t\t\tnum_sequence = torch.tensor (idxs, dtype=torch.long)\n",
        "\t \t\t#2. Prepare the target labels using something like: `targets = torch.tensor([self.tags.numberize(t) for t in tags], dtype=torch.long)`\n",
        "\t\t\ttaggela = [tag for word, tag in sentence]\n",
        "\n",
        "\t\t\ttargets = torch.tensor([self.tags.numberize(t) for t in taggela], dtype=torch.long)\n",
        "\n",
        "\t\t\t#print(\"tags\")\n",
        "\t \t\t#3. Call `self.zero_grad()` to clear any accumulated gradients from the previous update.\n",
        "\t\t\tself.zero_grad()\n",
        "\t\t\t#4. Pass the prepared sequence into the model by doing `self(sentence)` to obtain scores. This automatically calls forward().\n",
        "\t\t\tscores = self(num_sequence)\n",
        "\t\t\t#5. Calculate loss, passing in the output scores and the true target labels.\n",
        "\t\t\tloss = loss_function(scores, targets)\n",
        "\t\t\t#6. Call `loss.backward()` to compute gradients.\n",
        "\t\t\tloss.backward()\n",
        "\t\t\t#7. Apply gradient clipping to prevent exploding gradients. Use `torch.nn.utils.clip_grad_norm_()` with `self.parameters()` and a `max_norm` of 5.0.\n",
        "\t\t\ttorch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=5.0)\n",
        "\t\t\t#8. Call `optimizer.step()` to update the model parameters using the computed gradients.\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\t#9. Add `loss.item() * len(targets)` to `total_loss`.\n",
        "\t\t\ttotal_loss += loss.item() * len(targets)\n",
        "\t\t\t#10. Add `len(targets)` to `total_tokens`.\n",
        "\t\t\ttotal_tokens += len(targets)\n",
        "\t\t\t#print(\"for loop 2 \")\n",
        "\t\tnew_time = time.time()\n",
        "\t \t#5. Compute the average loss per token by dividing `total_loss / total_tokens`.\n",
        "\t\tavg_loss = total_loss/total_tokens\n",
        "\t\t#6. For debugging, it will be helpful to print the average loss per token and the runtime after each epoch. Average loss per token should always decrease epoch to epoch.\n",
        "\t\tprint(f\"avg loss per token: {avg_loss};\t\truntime: {new_time-curr_time}\")\n",
        "\n",
        "\n",
        "\t#raise NotImplementedError"
      ],
      "metadata": {
        "id": "4uNfBMQxFxej"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%add_to BiLSTMTagger\n",
        "\"\"\"     TODO: Iterating over the sentences in the data, calculate POS tagging accuracy.\n",
        "                * Use `self.eval()` and `with torch.no_grad()` so that the model is not trained during evaluation.\n",
        "                * Prepare the sequence and target labels as in fit().\n",
        "                * Use self.predict() to get the predicted tags, and then check if it matches the real next character found in the data.\n",
        "                * Divide the total correct predictions by the total number of tokens to get the final accuracy.\"\"\"\n",
        "def evaluate(self, data):\n",
        "  total_correct = 0\n",
        "  total_tokens = 0\n",
        "  self.eval()\n",
        "  with torch.no_grad():\n",
        "    for sentence in data:\n",
        "      idxs = [self.words.numberize(word.lower()) for word, tag in sentence] # prepped like before\n",
        "      num_sequence = torch.tensor (idxs, dtype=torch.long)\n",
        "\n",
        "      tags = [tag for word, tag in sentence]\n",
        "      target = torch.tensor([self.tags.numberize(t) for t in tags], dtype=torch.long)\n",
        "\n",
        "      predicted = self.predict(self(num_sequence))\n",
        "\n",
        "      #print predicted tag\n",
        "      denumber_predicted: list[str] = []\n",
        "      for i in predicted:\n",
        "        denumber_predicted.append(self.tags.denumberize(i))\n",
        "      print(denumber_predicted)\n",
        "      print(sentence)\n",
        "\n",
        "      total_correct += (predicted == target).sum().item()\n",
        "      total_tokens += len(target)\n",
        "\n",
        "    return total_correct / total_tokens"
      ],
      "metadata": {
        "id": "tlJSUs6bGROz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[1 point]** Report **loss per epoch** and **accuracy**. For full credit, accuracy must be at least 92% on val.pos and test.pos. Include your **saved model** in the submission files.\n",
        "- Because the dataset is small, scores from randomly initialized runs can have greater variance. If you're close to this score (e.g., 90%), there is likely not a bug and instead you had an unlucky run. You can run it again if you have time or comment that you believe the code is correct.\n"
      ],
      "metadata": {
        "id": "RCDNyNOjHZPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import read\n",
        "import utils\n",
        "def main():\n",
        "\ttrain = read_pos_file(\"/content/nlp_hw2/data/train.pos\")\n",
        "\tval = read_pos_file(\"/content/nlp_hw2/data/val.pos\")\n",
        "\ttest = read_pos_file(\"/content/nlp_hw2/data/test.pos\")\n",
        "\n",
        "\tmodel = BiLSTMTagger(train, embedding_dim=128, hidden_dim=256)\n",
        "\tprint(f\"Is '<UNK>' in model.words.sym2num? {'<UNK>' in model.words.sym2num}\")\n",
        "\tif '<UNK>' in model.words.sym2num:\n",
        "\t\tprint(f\"Index of '<UNK>': {model.words.sym2num['<UNK>']}\")\n",
        "\tprint(f\"Size of word vocabulary: {len(model.words.sym2num)}\")\n",
        "\tprint(f\"Size of tag vocabulary: {len(model.tags.sym2num)}\")\n",
        "\tprint(\"--- End Debugging Vocab in main ---\\n\")\n",
        "\n",
        "\tmodel.fit(train, lr=0.01, epochs=5)\n",
        "\tprint(model.evaluate(val))\n",
        "\tprint()\n",
        "\tprint(model.evaluate(test))\n",
        "\ttorch.save(model, \"my_model.pt\")\n",
        "\n",
        "  #raise NotImplementedError\n",
        "\n",
        "main()\n",
        "\n",
        "\"\"\"TODO: (reference HW1 part3.py)\n",
        "\t  * Use read_pos_file() from utils.py to read train.pos, val.pos, and test.pos.\n",
        "\t  * Initialize the model with training data, embedding dim 128, and hidden dim 256.\n",
        "\t  * Train the model, calling fit(), on the training data.\n",
        "\t  * Test the model, calling evaluate(), on the validation and test data.\n",
        "\t  * Predict outputs for the first ten examples in test.pos.\n",
        "\t  * Remove all instances of `raise NotImplementedError`!\"\"\""
      ],
      "metadata": {
        "id": "a2DrvtH2GYqC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9619bb2-65ad-4e35-a70c-51b2682d1dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<utils.Vocab object at 0x7f6d7eac25d0>\n",
            "Is '<UNK>' in model.words.sym2num? True\n",
            "Index of '<UNK>': 0\n",
            "Size of word vocabulary: 352\n",
            "Size of tag vocabulary: 34\n",
            "--- End Debugging Vocab in main ---\n",
            "\n",
            "avg loss per token: 0.537896096710341;\t\truntime: 3.944688320159912\n",
            "avg loss per token: 0.12891171719809197;\t\truntime: 3.901350736618042\n",
            "avg loss per token: 0.056285242120767166;\t\truntime: 4.449092388153076\n",
            "avg loss per token: 0.032563992935914866;\t\truntime: 3.887416362762451\n",
            "avg loss per token: 0.030644760971469294;\t\truntime: 3.8610825538635254\n",
            "['DT', 'NN', 'MD', 'VB', 'CD', 'RB', 'NN', 'PUNC']\n",
            "[('the', 'DT'), ('flight', 'NN'), ('should', 'MD'), ('be', 'VB'), ('eleven', 'CD'), ('a.m', 'RB'), ('tomorrow', 'NN'), ('.', 'PUNC')]\n",
            "['PRP', 'MD', 'VB', 'PRP', 'TO', 'VB', 'DT', 'NN', 'IN', 'NNP', 'NNP', 'CC', 'PRP', 'MD', 'VB', 'DT', 'NN', 'WDT', 'VBZ', 'NN', 'PUNC']\n",
            "[('i', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('it', 'PRP'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('stop', 'NN'), ('in', 'IN'), ('new', 'NNP'), ('york', 'NNP'), ('and', 'CC'), ('i', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('a', 'DT'), ('flight', 'NN'), ('that', 'WDT'), ('serves', 'VBZ'), ('breakfast', 'NN'), ('.', 'PUNC')]\n",
            "['WDT', 'IN', 'DT', 'VBP', 'NN', 'PUNC']\n",
            "[('which', 'WDT'), ('of', 'IN'), ('these', 'DT'), ('serve', 'VBP'), ('dinner', 'NN'), ('?', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'IN', 'NNP', 'PUNC']\n",
            "[('which', 'WDT'), ('ones', 'NNS'), ('stop', 'VBP'), ('in', 'IN'), ('nashville', 'NNP'), ('?', 'PUNC')]\n",
            "['VBP', 'EX', 'DT', 'NNS', 'VBG', 'IN', 'CD', 'RB', 'PUNC']\n",
            "[('are', 'VBP'), ('there', 'EX'), ('any', 'DT'), ('flights', 'NNS'), ('arriving', 'VBG'), ('after', 'IN'), ('eleven', 'CD'), ('a.m', 'RB'), ('?', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'PRP', 'VB', 'IN', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('do', 'VBP'), ('you', 'PRP'), ('have', 'VB'), ('from', 'IN'), ('ontario', 'NNP'), ('?', 'PUNC')]\n",
            "['WRB', 'VBZ', 'DT', 'NN', 'VB', 'PUNC']\n",
            "[('where', 'WRB'), ('does', 'VBZ'), ('this', 'DT'), ('flight', 'NN'), ('stop', 'VB'), ('?', 'PUNC')]\n",
            "['WDT', 'NN', 'IN', 'NN', 'VBZ', 'NNP', 'POS', 'NN', 'CD', 'NN', 'CD', 'PUNC']\n",
            "[('what', 'WDT'), ('type', 'NN'), ('of', 'IN'), ('aircraft', 'NN'), ('is', 'VBZ'), ('alaska', 'NNP'), (\"'s\", 'POS'), ('flight', 'NN'), ('two', 'CD'), ('eighty', 'CD'), ('two', 'CD'), ('?', 'PUNC')]\n",
            "['VBZ', 'DT', 'JJ', 'NNP', 'NN', 'PUNC']\n",
            "[('does', 'VBZ'), ('the', 'DT'), ('other', 'JJ'), ('american', 'NNP'), ('flight', 'NN'), ('?', 'PUNC')]\n",
            "['PRP', 'MD', 'VB', 'TO', 'VB', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('i', 'PRP'), (\"'d\", 'MD'), ('like', 'VB'), ('to', 'TO'), ('fly', 'VB'), ('from', 'IN'), ('tampa', 'NNP'), ('to', 'TO'), ('montreal', 'NNP'), ('.', 'PUNC')]\n",
            "['IN', 'NNP', 'PRP', 'MD', 'VB', 'TO', 'VB', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "[('on', 'IN'), ('tuesday', 'NNP'), ('i', 'PRP'), (\"'d\", 'MD'), ('like', 'VB'), ('to', 'TO'), ('fly', 'VB'), ('from', 'IN'), ('detroit', 'NNP'), ('to', 'TO'), ('saint', 'NNP'), ('petersburg', 'NNP'), ('.', 'PUNC')]\n",
            "['NN', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'IN', 'CD', 'NNP', 'PUNC']\n",
            "[('flight', 'NN'), ('from', 'IN'), ('long', 'NNP'), ('beach', 'NNP'), ('to', 'TO'), ('columbus', 'NNP'), ('on', 'IN'), ('twenty', 'CD'), ('seventh', 'JJ'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'IN', 'DT', 'NN', 'PUNC']\n",
            "[('which', 'WDT'), ('ones', 'NNS'), ('leave', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('?', 'PUNC')]\n",
            "['WDT', 'IN', 'DT', 'NNS', 'VBP', 'NN', 'PUNC']\n",
            "[('which', 'WDT'), ('of', 'IN'), ('these', 'DT'), ('flights', 'NNS'), ('serve', 'VBP'), ('dinner', 'NN'), ('?', 'PUNC')]\n",
            "['WDT', 'VBZ', 'JJ', 'PUNC']\n",
            "[('which', 'WDT'), ('is', 'VBZ'), ('last', 'JJ'), ('?', 'PUNC')]\n",
            "['VB', 'PRP', 'PDT', 'DT', 'NNS', 'IN', 'CD', 'NN', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('all', 'PDT'), ('the', 'DT'), ('fares', 'NNS'), ('from', 'IN'), ('one', 'CD'), ('way', 'NN'), ('fares', 'NNS'), ('from', 'IN'), ('tacoma', 'NNP'), ('to', 'TO'), ('montreal', 'NNP'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'VBP', 'PUNC']\n",
            "[('which', 'WDT'), ('flights', 'NNS'), ('serve', 'VBP'), ('breakfast', 'NN'), ('?', 'PUNC')]\n",
            "['WP', 'NN', 'NN', 'RB', 'NNP', 'NNS', 'IN', 'NNP', 'CC', 'NNP', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('airline', 'NN'), ('provides', 'VBZ'), ('only', 'RB'), ('connecting', 'VBG'), ('flights', 'NNS'), ('between', 'IN'), ('denver', 'NNP'), ('and', 'CC'), ('san', 'NNP'), ('francisco', 'NNP'), ('?', 'PUNC')]\n",
            "['VB', 'DT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('list', 'VB'), ('all', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('burbank', 'NNP'), ('to', 'TO'), ('denver', 'NNP'), ('.', 'PUNC')]\n",
            "['VBP', 'EX', 'DT', 'NNS', 'IN', 'NNP', 'CD', 'CD', 'CD', 'CD', 'PUNC']\n",
            "[('are', 'VBP'), ('there', 'EX'), ('any', 'DT'), ('stopovers', 'NNS'), ('for', 'IN'), ('delta', 'NNP'), ('one', 'CD'), ('seven', 'CD'), ('three', 'CD'), ('nine', 'CD'), ('?', 'PUNC')]\n",
            "['WP', 'VBZ', 'VBN', 'PRP', 'DT', 'PUNC']\n",
            "[('what', 'WP'), ('is', 'VBZ'), ('m', 'SYM'), ('i', 'SYM'), ('a', 'SYM'), ('?', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('leave', 'VBP'), ('from', 'IN'), ('phoenix', 'NNP'), ('to', 'TO'), ('anywhere', 'RB'), ('?', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'EX', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('are', 'VBP'), ('there', 'EX'), ('from', 'IN'), ('houston', 'NNP'), ('to', 'TO'), ('dallas', 'NNP'), ('?', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'EX', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('are', 'VBP'), ('there', 'EX'), ('from', 'IN'), ('memphis', 'NNP'), ('to', 'TO'), ('las', 'NNP'), ('vegas', 'NNP'), ('?', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('cleveland', 'NNP'), ('to', 'TO'), ('miami', 'NNP'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('on', 'IN'), ('sunday', 'NNP'), ('from', 'IN'), ('tampa', 'NNP'), ('to', 'TO'), ('charlotte', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'NNS', 'WDT', 'VBP', 'IN', 'DT', 'NN', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('flights', 'NNS'), ('that', 'WDT'), ('leave', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('.', 'PUNC')]\n",
            "['NNS', 'VBG', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'CC', 'NNP', 'NNP', 'NNP', 'IN', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('arriving', 'VBG'), ('in', 'IN'), ('las', 'NNP'), ('vegas', 'NNP'), ('from', 'IN'), ('memphis', 'NNP'), ('and', 'CC'), ('new', 'NNP'), ('york', 'NNP'), ('city', 'NNP'), ('on', 'IN'), ('sunday', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'NNS', 'VBG', 'IN', 'CD', 'NNS', 'IN', 'DT', 'JJ', 'PUNC']\n",
            "[('display', 'VB'), ('flights', 'NNS'), ('arriving', 'VBG'), ('within', 'IN'), ('thirty', 'CD'), ('minutes', 'NNS'), ('of', 'IN'), ('each', 'DT'), ('other', 'JJ'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NN', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('fare', 'NN'), ('.', 'PUNC')]\n",
            "['WP', 'VBP', 'DT', 'JJS', 'NN', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WP'), (\"'s\", 'VBZ'), ('the', 'DT'), ('earliest', 'JJS'), ('flight', 'NN'), ('from', 'IN'), ('dallas', 'NNP'), ('to', 'TO'), ('houston', 'NNP'), ('?', 'PUNC')]\n",
            "['IN', 'NNP', 'TO', 'NNP', 'IN', 'DT', 'NN', 'PUNC']\n",
            "[('from', 'IN'), ('toronto', 'NNP'), ('to', 'TO'), ('atlanta', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('afternoon', 'NN'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('kansas', 'NNP'), ('city', 'NNP'), ('to', 'TO'), ('denver', 'NNP'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('washington', 'NNP'), ('d', 'NNP'), ('c', 'NNP'), ('to', 'TO'), ('toronto', 'NNP'), ('.', 'PUNC')]\n",
            "['VBP', 'DT', 'IN', 'DT', 'IN', 'NNP', 'NNP', 'PUNC']\n",
            "[('are', 'VBP'), ('any', 'DT'), ('of', 'IN'), ('those', 'DT'), ('on', 'IN'), ('american', 'NNP'), ('airlines', 'NNP'), ('?', 'PUNC')]\n",
            "['WDT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'IN', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('montreal', 'NNP'), ('to', 'TO'), ('las', 'NNP'), ('vegas', 'NNP'), ('on', 'IN'), ('saturday', 'NNP'), ('.', 'PUNC')]\n",
            "['WP', 'VBZ', 'DT', 'NN', 'PUNC']\n",
            "[('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('price', 'NN'), ('?', 'PUNC')]\n",
            "['IN', 'NNP', 'PUNC']\n",
            "[('on', 'IN'), ('thursday', 'NNP'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('cleveland', 'NNP'), ('to', 'TO'), ('kansas', 'NNP'), ('city', 'NNP'), ('.', 'PUNC')]\n",
            "['NN', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'JJ', 'NN', 'PUNC']\n",
            "[('price', 'NN'), ('from', 'IN'), ('cleveland', 'NNP'), ('to', 'TO'), ('kansas', 'NNP'), ('city', 'NNP'), ('round', 'JJ'), ('trip', 'NN'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('ontario', 'NNP'), ('to', 'TO'), ('florida', 'NNP'), ('.', 'PUNC')]\n",
            "['JJ', 'NN', 'JJ', 'NN', 'NN', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('first', 'JJ'), ('class', 'NN'), ('round', 'JJ'), ('trip', 'NN'), ('airfare', 'NN'), ('from', 'IN'), ('indianapolis', 'NNP'), ('to', 'TO'), ('memphis', 'NNP'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('boston', 'NNP'), ('to', 'TO'), ('pittsburgh', 'NNP'), ('.', 'PUNC')]\n",
            "['WP', 'VBP', 'DT', 'JJS', 'PUNC']\n",
            "[('what', 'WP'), (\"'s\", 'VBZ'), ('the', 'DT'), ('cheapest', 'JJS'), ('?', 'PUNC')]\n",
            "['NN', 'NN', 'SYM', 'SYM', 'SYM', 'CD', 'CD', 'PUNC']\n",
            "[('restriction', 'NN'), ('code', 'NN'), ('a', 'SYM'), ('p', 'SYM'), ('slash', 'SYM'), ('five', 'CD'), ('seven', 'CD'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('new', 'NNP'), ('york', 'NNP'), ('to', 'TO'), ('miami', 'NNP'), ('.', 'PUNC')]\n",
            "['VBG', 'IN', 'CD', 'RB', 'PUNC']\n",
            "[('departing', 'VBG'), ('before', 'IN'), ('ten', 'CD'), ('a.m', 'RB'), ('.', 'PUNC')]\n",
            "['JJS', 'NN', 'PUNC']\n",
            "[('cheapest', 'JJS'), ('fare', 'NN'), ('.', 'PUNC')]\n",
            "['VBG', 'IN', 'CD', 'RB', 'PUNC']\n",
            "[('departing', 'VBG'), ('after', 'IN'), ('six', 'CD'), ('p.m', 'RB'), ('.', 'PUNC')]\n",
            "['VBG', 'NNP', 'IN', 'CD', 'RB', 'PUNC']\n",
            "[('departing', 'VBG'), ('wednesday', 'NNP'), ('after', 'IN'), ('five', 'CD'), (\"o'clock\", 'RB'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'NN', 'NN', 'IN', 'NNP', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('ground', 'NN'), ('transportation', 'NN'), ('in', 'IN'), ('westchester', 'NNP'), ('county', 'NNP'), ('.', 'PUNC')]\n",
            "['NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('newark', 'NNP'), ('to', 'TO'), ('cleveland', 'NNP'), ('.', 'PUNC')]\n",
            "['NN', 'NNP', 'NNP', 'NNP', 'PUNC']\n",
            "[('airline', 'NNP'), ('t', 'NNP'), ('w', 'NNP'), ('a', 'NNP'), ('.', 'PUNC')]\n",
            "['NNS', 'PUNC']\n",
            "[('fares', 'NNS'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNS', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('fares', 'NNS'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('fares', 'NNS'), ('from', 'IN'), ('dallas', 'NNP'), ('to', 'TO'), ('san', 'NNP'), ('francisco', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNS', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('san', 'NNP'), ('diego', 'NNP'), ('to', 'TO'), ('san', 'NNP'), ('francisco', 'NNP'), ('on', 'IN'), ('southwest', 'NNP'), ('airlines', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('restrictions', 'NNS'), ('.', 'PUNC')]\n",
            "0.9694989106753813\n",
            "\n",
            "['DT', 'NN', 'MD', 'VB', 'IN', 'CD', 'RB', 'NN', 'PUNC']\n",
            "[('the', 'DT'), ('flight', 'NN'), ('should', 'MD'), ('arrive', 'VB'), ('at', 'IN'), ('eleven', 'CD'), ('a.m', 'RB'), ('tomorrow', 'NN'), ('.', 'PUNC')]\n",
            "['PRP', 'MD', 'VB', 'TO', 'VB', 'DT', 'NN', 'WDT', 'VBZ', 'IN', 'NNP', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "[('i', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('find', 'VB'), ('a', 'DT'), ('flight', 'NN'), ('that', 'WDT'), ('goes', 'VBZ'), ('from', 'IN'), ('la', 'NNP'), ('guardia', 'NNP'), ('airport', 'NN'), ('to', 'TO'), ('san', 'NNP'), ('jose', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('newark', 'NNP'), ('to', 'TO'), ('los', 'NNP'), ('angeles', 'NNP'), ('.', 'PUNC')]\n",
            "['WDT', 'NN', 'VBZ', 'DT', 'PUNC']\n",
            "[('what', 'WDT'), ('airline', 'NN'), ('is', 'VBZ'), ('this', 'DT'), ('?', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('t', 'NNP'), ('w', 'NNP'), ('a', 'NNP'), ('flight', 'NN'), ('.', 'PUNC')]\n",
            "['PRP', 'MD', 'VB', 'TO', 'VB', 'TO', 'NNP', 'PUNC']\n",
            "[('i', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('travel', 'VB'), ('to', 'TO'), ('westchester', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'NNP', 'NNP', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('list', 'VB'), ('american', 'NNP'), ('airlines', 'NNP'), ('flights', 'NNS'), ('from', 'IN'), ('new', 'NNP'), ('york', 'NNP'), ('newark', 'NNP'), ('to', 'TO'), ('nashville', 'NNP'), ('.', 'PUNC')]\n",
            "['UH', 'PUNC']\n",
            "[('thanks', 'UH'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'EX', 'IN', 'NNP', 'TO', 'NNP', 'NN', 'NN', 'WDT', 'VBP', 'NN', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('are', 'VBP'), ('there', 'EX'), ('from', 'IN'), ('nashville', 'NNP'), ('to', 'TO'), ('houston', 'NNP'), ('tomorrow', 'NN'), ('evening', 'NN'), ('that', 'WDT'), ('serve', 'VBP'), ('dinner', 'NN'), ('?', 'PUNC')]\n",
            "['PRP', 'MD', 'VB', 'TO', 'VB', 'JJ', 'NNP', 'PUNC']\n",
            "[('i', 'PRP'), (\"'d\", 'MD'), ('like', 'VB'), ('to', 'TO'), ('fly', 'VB'), ('next', 'JJ'), ('friday', 'NNP'), ('.', 'PUNC')]\n",
            "['WDT', 'IN', 'DT', 'VBP', 'JJ', 'PUNC']\n",
            "[('which', 'WDT'), ('of', 'IN'), ('those', 'DT'), ('are', 'VBP'), ('nonstop', 'JJ'), ('?', 'PUNC')]\n",
            "['PRP', 'VBP', 'DT', 'NN', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('i', 'PRP'), ('want', 'VBP'), ('a', 'DT'), ('flight', 'NN'), ('from', 'IN'), ('ontario', 'NNP'), ('to', 'TO'), ('chicago', 'NNP'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'VB', 'JJ', 'IN', 'DT', 'NN', 'PUNC']\n",
            "[('which', 'WDT'), ('ones', 'NNS'), ('arrive', 'VBP'), ('early', 'RB'), ('in', 'IN'), ('the', 'DT'), ('day', 'NN'), ('?', 'PUNC')]\n",
            "['WDT', 'IN', 'DT', 'NNS', 'VBP', 'RBS', 'PUNC']\n",
            "[('which', 'WDT'), ('of', 'IN'), ('these', 'DT'), ('flights', 'NNS'), ('depart', 'VBP'), ('latest', 'RBS'), ('?', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('baltimore', 'NNP'), ('to', 'TO'), ('oakland', 'NNP'), ('.', 'PUNC')]\n",
            "['VBP', 'EX', 'DT', 'JJ', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('are', 'VBP'), ('there', 'EX'), ('any', 'DT'), ('other', 'JJ'), ('fares', 'NNS'), ('from', 'IN'), ('tacoma', 'NNP'), ('to', 'TO'), ('montreal', 'NNP'), ('?', 'PUNC')]\n",
            "['WDT', 'VBZ', 'DT', 'JJS', 'PUNC']\n",
            "[('which', 'WDT'), ('is', 'VBZ'), ('the', 'DT'), ('latest', 'JJS'), ('?', 'PUNC')]\n",
            "['WDT', 'NN', 'IN', 'NN', 'VBZ', 'NNP', 'IN', 'DT', 'NNS', 'PUNC']\n",
            "[('what', 'WDT'), ('type', 'NN'), ('of', 'IN'), ('aircraft', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('on', 'IN'), ('those', 'DT'), ('flights', 'NNS'), ('?', 'PUNC')]\n",
            "['WP', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'NNP', 'NNP', 'NN', 'CD', 'CD', 'PUNC']\n",
            "[('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('lowest', 'JJS'), ('fare', 'NN'), ('for', 'IN'), ('united', 'NNP'), ('airlines', 'NNP'), ('flight', 'NN'), ('four', 'CD'), ('thirty', 'CD'), ('?', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('salt', 'NNP'), ('lake', 'NNP'), ('city', 'NNP'), ('to', 'TO'), ('phoenix', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'NNS', 'PUNC']\n",
            "[('show', 'VB'), ('flights', 'NNS'), ('.', 'PUNC')]\n",
            "['WP', 'VBZ', 'VBN', 'NNP', 'NNP', 'PUNC']\n",
            "[('what', 'WP'), ('is', 'VBZ'), ('b', 'SYM'), ('n', 'SYM'), ('a', 'SYM'), ('?', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'EX', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('are', 'VBP'), ('there', 'EX'), ('from', 'IN'), ('newark', 'NNP'), ('to', 'TO'), ('tampa', 'NNP'), ('?', 'PUNC')]\n",
            "['WDT', 'NNS', 'VBP', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('are', 'VBP'), ('from', 'IN'), ('memphis', 'NNP'), ('to', 'TO'), ('las', 'NNP'), ('vegas', 'NNP'), ('?', 'PUNC')]\n",
            "['VB', 'NN', 'IN', 'NNP', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'CC', 'NNP', 'NNP', 'NNP', 'CD', 'CD', 'CD', 'PUNC']\n",
            "[('show', 'VB'), ('cost', 'NN'), ('of', 'IN'), ('u', 'NNP'), ('s', 'NNP'), ('air', 'NNP'), ('fifteen', 'CD'), ('twenty', 'CD'), ('three', 'CD'), ('and', 'CC'), ('u', 'NNP'), ('s', 'NNP'), ('air', 'NNP'), ('seven', 'CD'), ('eight', 'CD'), ('one', 'CD'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('charlotte', 'NNP'), ('to', 'TO'), ('baltimore', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'NNS', 'WDT', 'VBP', 'IN', 'DT', 'NN', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('flights', 'NNS'), ('that', 'WDT'), ('leave', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('.', 'PUNC')]\n",
            "['NN', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'NNP', 'PUNC']\n",
            "[('change', 'VB'), ('newark', 'NNP'), ('city', 'NNP'), ('to', 'TO'), ('new', 'NNP'), ('york', 'NNP'), ('city', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'NNS', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'IN', 'NNP', 'NN', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('flights', 'NNS'), ('from', 'IN'), ('los', 'NNP'), ('angeles', 'NNP'), ('to', 'TO'), ('pittsburgh', 'NNP'), ('on', 'IN'), ('monday', 'NNP'), ('evening', 'NN'), ('.', 'PUNC')]\n",
            "['VB', 'DT', 'NNS', 'PUNC']\n",
            "[('show', 'VB'), ('all', 'DT'), ('fares', 'NNS'), ('.', 'PUNC')]\n",
            "['WP', 'VBP', 'DT', 'JJS', 'NN', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WP'), (\"'s\", 'VBZ'), ('the', 'DT'), ('latest', 'JJS'), ('flight', 'NN'), ('from', 'IN'), ('houston', 'NNP'), ('to', 'TO'), ('dallas', 'NNP'), ('?', 'PUNC')]\n",
            "['IN', 'NNP', 'TO', 'NNP', 'IN', 'DT', 'NN', 'PUNC']\n",
            "[('from', 'IN'), ('toronto', 'NNP'), ('to', 'TO'), ('atlanta', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('afternoon', 'NN'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('denver', 'NNP'), ('to', 'TO'), ('salt', 'NNP'), ('lake', 'NNP'), ('city', 'NNP'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('atlanta', 'NNP'), ('to', 'TO'), ('washington', 'NNP'), ('.', 'PUNC')]\n",
            "['WP', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('price', 'NN'), ('of', 'IN'), ('flights', 'NNS'), ('from', 'IN'), ('indianapolis', 'NNP'), ('to', 'TO'), ('memphis', 'NNP'), ('?', 'PUNC')]\n",
            "['WDT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'IN', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('phoenix', 'NNP'), ('to', 'TO'), ('las', 'NNP'), ('vegas', 'NNP'), ('on', 'IN'), ('saturday', 'NNP'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('detroit', 'NNP'), ('to', 'TO'), ('saint', 'NNP'), ('petersburg', 'NNP'), ('.', 'PUNC')]\n",
            "['WDT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('minneapolis', 'NNP'), ('to', 'TO'), ('pittsburgh', 'NNP'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'IN', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('cleveland', 'NNP'), ('to', 'TO'), ('kansas', 'NNP'), ('city', 'NNP'), ('on', 'IN'), ('monday', 'NNP'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('kansas', 'NNP'), ('city', 'NNP'), ('to', 'TO'), ('cleveland', 'NNP'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('ontario', 'NNP'), ('to', 'TO'), ('orlando', 'NNP'), ('.', 'PUNC')]\n",
            "['JJS', 'NN', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('cheapest', 'JJS'), ('flight', 'NN'), ('from', 'IN'), ('indianapolis', 'NNP'), ('to', 'TO'), ('memphis', 'NNP'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('pittsburgh', 'NNP'), ('to', 'TO'), ('newark', 'NNP'), ('.', 'PUNC')]\n",
            "['WP', 'VBZ', 'DT', 'RBS', 'PUNC']\n",
            "[('what', 'WP'), ('does', 'VBZ'), ('that', 'DT'), ('mean', 'NN'), ('?', 'PUNC')]\n",
            "['JJ', 'NN', 'NNS', 'VBP', 'NN', 'NN', 'SYM', 'SYM', 'SYM', 'CD', 'CD', 'PUNC']\n",
            "[('round', 'JJ'), ('trip', 'NN'), ('flights', 'NNS'), ('without', 'IN'), ('restriction', 'NN'), ('code', 'NN'), ('a', 'SYM'), ('p', 'SYM'), ('slash', 'SYM'), ('five', 'CD'), ('seven', 'CD'), ('.', 'PUNC')]\n",
            "['JJ', 'NN', 'NN', 'PUNC']\n",
            "[('first', 'JJ'), ('class', 'NN'), ('fare', 'NN'), ('.', 'PUNC')]\n",
            "['JJ', 'NN', 'PUNC']\n",
            "[('first', 'JJ'), ('class', 'NN'), ('.', 'PUNC')]\n",
            "['NN', 'NN', 'PUNC']\n",
            "[('return', 'NN'), ('flight', 'NN'), ('.', 'PUNC')]\n",
            "['VBG', 'IN', 'CD', 'RB', 'PUNC']\n",
            "[('departing', 'VBG'), ('after', 'IN'), ('five', 'CD'), ('p.m', 'RB'), ('.', 'PUNC')]\n",
            "['IN', 'NNS', 'IN', 'CD', 'RB', 'PUNC']\n",
            "[('on', 'IN'), ('wednesdays', 'NNP'), ('after', 'IN'), ('five', 'CD'), ('p.m', 'RB'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'JJ', 'PUNC']\n",
            "[('flights', 'NNS'), ('from', 'IN'), ('westchester', 'NNP'), ('county', 'NNP'), ('to', 'TO'), ('san', 'NNP'), ('francisco', 'NNP'), ('daily', 'RB'), ('.', 'PUNC')]\n",
            "['NNP', 'TO', 'NNP', 'PUNC']\n",
            "[('newark', 'NNP'), ('to', 'TO'), ('cleveland', 'NNP'), ('.', 'PUNC')]\n",
            "['NNS', 'IN', 'NNP', 'NNP', 'NNP', 'PUNC']\n",
            "[('flights', 'NNS'), ('on', 'IN'), ('t', 'NNP'), ('w', 'NNP'), ('a', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'DT', 'NNS', 'PUNC']\n",
            "[('show', 'VB'), ('the', 'DT'), ('fares', 'NNS'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NN', 'NN', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('return', 'NN'), ('trips', 'NNS'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNS', 'IN', 'NNP', 'NNP', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('all', 'DT'), ('flights', 'NNS'), ('on', 'IN'), ('southwest', 'NNP'), ('airlines', 'NNP'), ('from', 'IN'), ('san', 'NNP'), ('diego', 'NNP'), ('to', 'TO'), ('san', 'NNP'), ('francisco', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'PRP', 'DT', 'NNS', 'IN', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'NNP', 'PUNC']\n",
            "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('san', 'NNP'), ('diego', 'NNP'), ('to', 'TO'), ('washington', 'NNP'), ('d', 'NNP'), ('c', 'NNP'), ('.', 'PUNC')]\n",
            "['VB', 'DT', 'NNP', 'PUNC']\n",
            "[('explain', 'VB'), ('the', 'DT'), ('restrictions', 'NNS'), ('.', 'PUNC')]\n",
            "0.9655172413793104\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TODO: (reference HW1 part3.py)\\n\\t  * Use read_pos_file() from utils.py to read train.pos, val.pos, and test.pos.\\n\\t  * Initialize the model with training data, embedding dim 128, and hidden dim 256.\\n\\t  * Train the model, calling fit(), on the training data.\\n\\t  * Test the model, calling evaluate(), on the validation and test data.\\n\\t  * Predict outputs for the first ten examples in test.pos.\\n\\t  * Remove all instances of `raise NotImplementedError`!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.download(\"my_model.pt\")"
      ],
      "metadata": {
        "id": "L9BWtFrmWCi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "['DT', 'NN', 'MD', 'VB', 'IN', 'CD', 'RB', 'NN', 'PUNC']\n",
        "\n",
        "[('the', 'DT'), ('flight', 'NN'), ('should', 'MD'), ('arrive', 'VB'), ('at', 'IN'), ('eleven', 'CD'), ('a.m', 'RB'), ('tomorrow', 'NN'), ('.', 'PUNC')]\n",
        "\n",
        "\n",
        "\n",
        "['PRP', 'MD', 'VB', 'TO', 'VB', 'DT', 'NN', 'WDT', 'VBZ', 'IN', 'NNP', 'NNP', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
        "\n",
        "[('i', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('find', 'VB'), ('a', 'DT'), ('flight', 'NN'), ('that', 'WDT'), ('goes', 'VBZ'), ('from', 'IN'), ('la', 'NNP'), ('guardia', 'NNP'), ('airport', 'NN'), ('to', 'TO'), ('san', 'NNP'), ('jose', 'NNP'), ('.', 'PUNC')]\n",
        "\n",
        "\n",
        "\n",
        "['VB', 'PRP', 'DT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
        "\n",
        "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('newark', 'NNP'), ('to', 'TO'), ('los', 'NNP'), ('angeles', 'NNP'), ('.', 'PUNC')]\n",
        "\n",
        "\n",
        "\n",
        "['WDT', 'NN', 'VBZ', 'DT', 'PUNC']\n",
        "\n",
        "[('what', 'WDT'), ('airline', 'NN'), ('is', 'VBZ'), ('this', 'DT'), ('?', 'PUNC')]\n",
        "\n",
        "\n",
        "\n",
        "['VB', 'PRP', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'PUNC']\n",
        "\n",
        "[('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('t', 'NNP'), ('w', 'NNP'), ('a', 'NNP'), ('flight', 'NN'), ('.', 'PUNC')]\n",
        "\n",
        "\n",
        "\n",
        "['PRP', 'MD', 'VB', 'TO', 'VB', 'TO', 'NNP', 'PUNC']\n",
        "\n",
        "[('i', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('travel', 'VB'), ('to', 'TO'), ('westchester', 'NNP'), ('.', 'PUNC')]\n",
        "\n",
        "\n",
        "\n",
        "['VB', 'NNP', 'NNP', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'TO', 'NNP', 'PUNC']\n",
        "\n",
        "[('list', 'VB'), ('american', 'NNP'), ('airlines', 'NNP'), ('flights', 'NNS'), ('from', 'IN'), ('new', 'NNP'), ('york', 'NNP'), ('newark', 'NNP'), ('to', 'TO'), ('nashville', 'NNP'), ('.', 'PUNC')]\n",
        "\n",
        "\n",
        "\n",
        "['UH', 'PUNC']\n",
        "\n",
        "[('thanks', 'UH'), ('.', 'PUNC')]\n",
        "\n",
        "\n",
        "\n",
        "['WDT', 'NNS', 'VBP', 'EX', 'IN', 'NNP', 'TO', 'NNP', 'NN', 'NN', 'WDT', 'VBP', 'NN', 'PUNC']\n",
        "\n",
        "[('what', 'WDT'), ('flights', 'NNS'), ('are', 'VBP'), ('there', 'EX'), ('from', 'IN'), ('nashville', 'NNP'), ('to', 'TO'), ('houston', 'NNP'), ('tomorrow', 'NN'), ('evening', 'NN'), ('that', 'WDT'), ('serve', 'VBP'), ('dinner', 'NN'), ('?', 'PUNC')]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "['PRP', 'MD', 'VB', 'TO', 'VB', 'JJ', 'NNP', 'PUNC']\n",
        "\n",
        "[('i', 'PRP'), (\"'d\", 'MD'), ('like', 'VB'), ('to', 'TO'), ('fly', 'VB'), ('next', 'JJ'), ('friday', 'NNP'), ('.', 'PUNC')]\n"
      ],
      "metadata": {
        "id": "Nb-LmfvP0M2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[2 points]** Free response: For the first 10 sentences of test.pos, **report the POS tags predicted by your model.**\n",
        "\n",
        "\n",
        "\n",
        "- What works well, and what doesn't?\n",
        "> It seems like the words are being properly tagged with only one error (that I spotted) amoung the first. However place names seem to be an area of struggle, and it has to be noted that it seems most of these sentences are to the point.\n",
        "\n",
        "- For words tagged incorrectly, why do you think it happens, and what tag do they tend to get?\n",
        ">\n",
        "\n",
        "One word that was mis-tagged was airport going from NN to NNP. In this it seems that they give the tage that most closely resembling the original. In this case it was NN (general noun) turning into NNP.\n",
        "\n",
        "- Think about micro- and macro-level tags (e.g., is it tagging NNS as NN, or VBD as NN, and which one is worse?).\n",
        "> It seems mistagging NN is higher, as thats the one that I spotted. Hoever NNS to NN isn't too bad as it's under the same general catergory of noun. However VBD to NN is much worse, as it's under a completely different meaning.\n"
      ],
      "metadata": {
        "id": "c-Z64TFuHqYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Probabilistic Context Free Grammar\n",
        "In this part, you will learn a PCFG given training data annotated with parse trees."
      ],
      "metadata": {
        "id": "TRcIs3YjIGf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import trees\n",
        "import fileinput\n",
        "import collections\n",
        "import re\n",
        "\"\"\"You should not need any other imports, but you may import anything that helps.\"\"\"\n",
        "\n",
        "counts = collections.defaultdict(collections.Counter)"
      ],
      "metadata": {
        "id": "0I5vMzJ4MGbn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[4 points]** Write code to read in trees and to count all the rules used in each tree. **Terminal nodes should be POS tags rather than words** to allow us to use the POS tagger. This means the grammar must contain a set of rules mapping each nonterminal POS tags in the PTB tag set to a terminal POS tag:\n",
        "```\n",
        "DT -> DT_t\n",
        "NN -> NN_t\n",
        "NNS -> NNS_t\n",
        "```\n"
      ],
      "metadata": {
        "id": "Tx8PLiCSK0Jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#I HATE TREES!!!!"
      ],
      "metadata": {
        "id": "AMBQR5x5j_m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_rules():\n",
        "\n",
        "  \"\"\"TODO: Collect all the tree branching rules used in the parses in train.trees, and count their frequencies.\n",
        "    * Use trees.Tree.from_str(), bottomup(), and other helpful functions from trees.py.\n",
        "    * Goal: end up with three dictionaries, counts, probs, and cfg.\n",
        "      * counts has entries count[LHS][RHS], like count[NP][(DT, NN)].\n",
        "      * probs has entries prob[LHS][RHS] = count[LHS][RHS] / sum(count[LHS].values())\n",
        "      * cfg simply has the rules of the grammar, stored using whichever structure is usable to you for your CKY implementation. For instance, indexing by RHS may be easier to look up for CKY (cfg[RHS][LHS].\n",
        "      * To include terminal words as POS_t (e.g., NN_t) as you're constructing the CFG:\n",
        "        if len(node.children) == 1: # terminal rules\n",
        "          rhs = (node.label.split('_')[-1]+'_t',) # rhs = (NN_t,)\n",
        "  \"\"\"\n",
        "\n",
        "  rule_counts = collections.defaultdict(lambda: collections.defaultdict(int)) # {rule_n : count}\n",
        "  rule_probability = collections.defaultdict(dict) # {rule_n : prob}\n",
        "  rule_cfg = collections.defaultdict(list) # {rule_n : cfg}\n",
        "\n",
        "\n",
        "  #for line in fileinput.input(\"/content/nlp_hw2/data/train.trees\"):\n",
        "  with open(\"/content/nlp_hw2/data/train.trees\", \"r\") as f:\n",
        "    #lines = f.read()\n",
        "    #print(lines) #<-- all trees\n",
        "    for line in f:\n",
        "      #print(line)\n",
        "      if line is None:\n",
        "        continue\n",
        "      tree = trees.Tree.from_str(line) # one tree\n",
        "      #print(tree)\n",
        "      #if tree.root is None: #done with this tree will move on\n",
        "      #  continue\n",
        "      #print(\"scary\")\n",
        "      for node in tree.bottomup(): #go to the bottom of the rules\n",
        "        if len(node.children) == 0 : # may need to just check if its zero\n",
        "          continue\n",
        "        lhs = node.label\n",
        "        if len(node.children) == 1: # terminal rules\n",
        "          rhs = (node.label.split('_')[-1]+'_t',)\n",
        "        else:\n",
        "          rhs = tuple(child.label for child in node.children)\n",
        "\n",
        "        rule_counts[lhs][rhs] += 1\n",
        "\n",
        "  #now for probabilites\n",
        "  #lhs_counts = collections.defaultdict(dict)\n",
        "  for lhs in rule_counts:\n",
        "      total = sum(rule_counts[lhs].values())\n",
        "      #lhs_counts[lhs] += count\n",
        "      for rhs in rule_counts[lhs]:\n",
        "        rule_probability[lhs][rhs] = rule_counts[lhs][rhs] / total\n",
        "        #rule_cfg[lhs].append(rhs)\n",
        "\n",
        "  #finally cfg\n",
        "  for lhs, counting in rule_counts.items():\n",
        "    for rhs in counting:\n",
        "      rule_cfg[rhs].append(lhs)\n",
        "\n",
        "  return rule_counts, rule_probability, rule_cfg\n",
        "  #raise NotImplementedError"
      ],
      "metadata": {
        "id": "QReJeAGHLMYz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[3 points]** Write code to compute the conditional probability of each rule and **print the PCFG in a readable format**, such as:\n",
        "```\n",
        "NP -> DT NN # 0.5\n",
        "NP -> DT NNS # 0.5\n",
        "```"
      ],
      "metadata": {
        "id": "EqcVdullK89D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_pcfg():\n",
        "  #computing the conditional probabaility of each rule\n",
        "  #print(\"begining\")\n",
        "  rule_counts, rule_probability, rule_cfg = count_rules()\n",
        "  #print(rule_counts)\n",
        "  #print()\n",
        "  #print(rule_probability)\n",
        "  #print()\n",
        "  #print(rule_cfg)\n",
        "\n",
        "  for lhs in rule_probability:\n",
        "    for rhs in rule_probability[lhs]:\n",
        "      print(f\"{lhs} -> {' '.join(rhs)} # {rule_probability[lhs][rhs]}\")\n",
        "\n",
        "  #get unique rules\n",
        "  unique_rules = set()\n",
        "  for lhs in rule_cfg:\n",
        "    for rhs in rule_cfg[lhs]:\n",
        "      unique_rules.add((lhs, rhs))\n",
        "  #print number of uniqe rules\n",
        "  print(f\"Number of unique rules: {len(unique_rules)}\")\n",
        "\n",
        "  #top 10 most frequent rules\n",
        "# Top 10 most frequent rules (LHS -> RHS)\n",
        "  all_rules_with_counts = [((lhs, rhs), count)\n",
        "                          for lhs in rule_counts\n",
        "                          for rhs, count in rule_counts[lhs].items()]\n",
        "\n",
        "  top_10_rules = sorted(all_rules_with_counts, key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "  print(\"\\nTop 10 Most Frequent Rules:\")\n",
        "  for (lhs, rhs), count in top_10_rules:\n",
        "      rhs_str = ' '.join(rhs)\n",
        "      print(f\"{lhs} -> {rhs_str} # Count: {count}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #top 6 highest-probability rules with left hand side NP\n",
        "  top_6_rules = sorted(rule_probability['NP'].items(), key=lambda x: x[1], reverse=True)[:6]\n",
        "  for rule in top_6_rules:\n",
        "    print(f\"{rule[0]} # {rule[1]}\")\n",
        "    #print(f\"{rule[0]} -> {' '.join(rule[1])} # {sum(rule[1].values())}\")\n",
        "\n",
        "\n",
        "\n",
        "  #printing pcfg in readable format\n",
        "  #raise NotImplementedError"
      ],
      "metadata": {
        "id": "dv8X83skLM-U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[3 points]** Running the code on train.trees, **report**:\n",
        "- How many unique rules are there?\n",
        "> 419\n",
        "- What are the top five most frequent rules, and how many times did each occur?\n",
        "> IN -> IN_t # Count: 482\n",
        ">\n",
        ">PUNC -> PUNC_t # Count: 469\n",
        ">\n",
        ">NP_NNP -> NNP_t # Count: 451\n",
        ">\n",
        ">NNP -> NNP_t # Count: 408\n",
        ">\n",
        ">NN -> NN_t # Count: 281\n",
        "- What are the top five highest-probability rules with left-hand side NP, and what are their probabilities?\n",
        "> ('NNP', 'NNP') # 0.1927536231884058\n",
        ">\n",
        ">('NP', 'NP*') # 0.11594202898550725\n",
        ">\n",
        ">('DT', 'NN') # 0.10144927536231885\n",
        ">\n",
        ">('DT', 'NNS') # 0.08550724637681159\n",
        ">\n",
        ">('DT', 'NP*') # 0.08405797101449275\n",
        "- **Free Response**: Did the most frequent rules surprise you? Why or why not?\n",
        "> Not really becuase nouns are one of the most frequent used word 'type' so I'm not surprise that they ended up frequenting it as much as they did\n"
      ],
      "metadata": {
        "id": "RwlFtXGULE_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  print_pcfg()\n",
        "main()"
      ],
      "metadata": {
        "id": "lbEBpSxPKwMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4237539e-3806-42e6-8a46-242cc4b7379d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VB -> VB_t # 1.0\n",
            "DT -> DT_t # 1.0\n",
            "NNS -> NNS_t # 1.0\n",
            "NP -> DT NNS # 0.08550724637681159\n",
            "NP -> NP NP* # 0.11594202898550725\n",
            "NP -> DT NN # 0.10144927536231885\n",
            "NP -> NP PP # 0.04057971014492753\n",
            "NP -> CD NNS # 0.002898550724637681\n",
            "NP -> NNP NNP # 0.1927536231884058\n",
            "NP -> NP_NNP NP # 0.013043478260869565\n",
            "NP -> NP SBAR # 0.01884057971014493\n",
            "NP -> NP_CD PP # 0.0014492753623188406\n",
            "NP -> DT JJ # 0.007246376811594203\n",
            "NP -> NNP NP* # 0.0463768115942029\n",
            "NP -> QP RB # 0.004347826086956522\n",
            "NP -> JJ NNP # 0.004347826086956522\n",
            "NP -> DT NP* # 0.08405797101449275\n",
            "NP -> CD RB # 0.030434782608695653\n",
            "NP -> PDT NP* # 0.004347826086956522\n",
            "NP -> NP VP # 0.005797101449275362\n",
            "NP -> CD NP* # 0.02318840579710145\n",
            "NP -> DT NX # 0.0014492753623188406\n",
            "NP -> NN NN # 0.013043478260869565\n",
            "NP -> NP_NNP NP_NNP # 0.007246376811594203\n",
            "NP -> NN NP* # 0.017391304347826087\n",
            "NP -> NNP JJ # 0.002898550724637681\n",
            "NP -> NNP POS # 0.002898550724637681\n",
            "NP -> NN NNS # 0.010144927536231883\n",
            "NP -> NP_NN NP* # 0.007246376811594203\n",
            "NP -> NP_NN PP # 0.0014492753623188406\n",
            "NP -> JJ NN # 0.015942028985507246\n",
            "NP -> CD CD # 0.002898550724637681\n",
            "NP -> NNP NN # 0.017391304347826087\n",
            "NP -> CD NN # 0.014492753623188406\n",
            "NP -> QP NN # 0.002898550724637681\n",
            "NP -> JJ NNS # 0.007246376811594203\n",
            "NP -> NP_NNP NP* # 0.008695652173913044\n",
            "NP -> NP ADJP_JJ # 0.0014492753623188406\n",
            "NP -> NP_DT PP # 0.002898550724637681\n",
            "NP -> NP_NNS NP* # 0.01884057971014493\n",
            "NP -> JJ NP* # 0.013043478260869565\n",
            "NP -> NP SBAR_S_VP # 0.0014492753623188406\n",
            "NP -> NP_NNS PP # 0.004347826086956522\n",
            "NP -> QP NNS # 0.010144927536231883\n",
            "NP -> NP NNS # 0.0014492753623188406\n",
            "NP -> NP_NNS VP # 0.002898550724637681\n",
            "NP -> JJS NP* # 0.005797101449275362\n",
            "NP -> WDT NNS # 0.0014492753623188406\n",
            "NP -> JJS NN # 0.011594202898550725\n",
            "NP -> CC NP* # 0.0014492753623188406\n",
            "NP -> NP NP # 0.002898550724637681\n",
            "NP -> NNP NNS # 0.0014492753623188406\n",
            "NP -> QP NP* # 0.0014492753623188406\n",
            "NP -> RB NP* # 0.0014492753623188406\n",
            "IN -> IN_t # 1.0\n",
            "NP_NNP -> NNP_t # 1.0\n",
            "PP -> IN NP_NNP # 0.34992679355783307\n",
            "PP -> TO NP_NNP # 0.232796486090776\n",
            "PP -> IN NP # 0.2884333821376281\n",
            "PP -> IN NP_PRP # 0.0014641288433382138\n",
            "PP -> TO NP # 0.0849194729136164\n",
            "PP -> IN NP_DT # 0.016105417276720352\n",
            "PP -> IN NP_NN # 0.013177159590043924\n",
            "PP -> IN ADJP_JJ # 0.0014641288433382138\n",
            "PP -> IN PP* # 0.0014641288433382138\n",
            "PP -> PP PP* # 0.0014641288433382138\n",
            "PP -> NP PP* # 0.0014641288433382138\n",
            "PP -> IN NP_NNPS # 0.0014641288433382138\n",
            "PP -> IN NP_NNS # 0.004392386530014641\n",
            "PP -> TO INTJ_UH # 0.0014641288433382138\n",
            "TO -> TO_t # 1.0\n",
            "WHNP_WDT -> WDT_t # 1.0\n",
            "VBP -> VBP_t # 1.0\n",
            "S_VP -> VBP PP # 0.014388489208633094\n",
            "S_VP -> VB NP # 0.1223021582733813\n",
            "S_VP -> TO VP # 0.1510791366906475\n",
            "S_VP -> MD VP # 0.014388489208633094\n",
            "S_VP -> VBZ VP* # 0.014388489208633094\n",
            "S_VP -> VB VP* # 0.5539568345323741\n",
            "S_VP -> VB NP_PRP # 0.014388489208633094\n",
            "S_VP -> VBP NP # 0.014388489208633094\n",
            "S_VP -> VBZ PP # 0.02158273381294964\n",
            "S_VP -> VBP NP_NN # 0.014388489208633094\n",
            "S_VP -> VBZ NP # 0.007194244604316547\n",
            "S_VP -> VB NP_NNS # 0.014388489208633094\n",
            "S_VP -> VBZ ADVP_RB # 0.007194244604316547\n",
            "S_VP -> VBP ADVP # 0.007194244604316547\n",
            "S_VP -> VP VP* # 0.014388489208633094\n",
            "S_VP -> VBP VP* # 0.007194244604316547\n",
            "S_VP -> VBZ NP_NN # 0.007194244604316547\n",
            "SBAR -> WHNP_WDT S_VP # 0.9090909090909091\n",
            "SBAR -> WHNP_WDT S # 0.045454545454545456\n",
            "SBAR -> WHNP SQ # 0.045454545454545456\n",
            "NP* -> PP SBAR # 0.011210762331838564\n",
            "NP* -> PP NP* # 0.1210762331838565\n",
            "NP* -> VP VP # 0.002242152466367713\n",
            "NP* -> VP SBAR # 0.004484304932735426\n",
            "NP* -> NNP NNP # 0.06053811659192825\n",
            "NP* -> PP PP # 0.242152466367713\n",
            "NP* -> PP NP # 0.013452914798206279\n",
            "NP* -> NN NN # 0.05605381165919283\n",
            "NP* -> JJ NP* # 0.02914798206278027\n",
            "NP* -> NNP NN # 0.006726457399103139\n",
            "NP* -> DT NNS # 0.006726457399103139\n",
            "NP* -> NN NP* # 0.02242152466367713\n",
            "NP* -> CD RB # 0.006726457399103139\n",
            "NP* -> CC NP* # 0.008968609865470852\n",
            "NP* -> JJ NN # 0.02242152466367713\n",
            "NP* -> PP NP_NN # 0.006726457399103139\n",
            "NP* -> PP VP # 0.02242152466367713\n",
            "NP* -> CD NN # 0.006726457399103139\n",
            "NP* -> CD NP* # 0.04484304932735426\n",
            "NP* -> NNP NP* # 0.0515695067264574\n",
            "NP* -> CD CD # 0.03811659192825112\n",
            "NP* -> NP_NN NP* # 0.004484304932735426\n",
            "NP* -> JJS NN # 0.01569506726457399\n",
            "NP* -> CC NP # 0.033632286995515695\n",
            "NP* -> X_TO PP # 0.002242152466367713\n",
            "NP* -> NP NP* # 0.004484304932735426\n",
            "NP* -> NN NNS # 0.0515695067264574\n",
            "NP* -> ADVP_RB NP # 0.004484304932735426\n",
            "NP* -> JJS NP* # 0.006726457399103139\n",
            "NP* -> SYM CD # 0.002242152466367713\n",
            "NP* -> SBAR PP # 0.002242152466367713\n",
            "NP* -> X_JJ NNS # 0.002242152466367713\n",
            "NP* -> NNS PP # 0.002242152466367713\n",
            "NP* -> CC NP_NNP # 0.008968609865470852\n",
            "NP* -> VP ADVP # 0.006726457399103139\n",
            "NP* -> CD NNS # 0.006726457399103139\n",
            "NP* -> NNP NNS # 0.011210762331838564\n",
            "NP* -> CC NNP # 0.006726457399103139\n",
            "NP* -> NN SYM # 0.004484304932735426\n",
            "NP* -> CC NP_NN # 0.002242152466367713\n",
            "NP* -> ADJP NN # 0.002242152466367713\n",
            "NP* -> RB RB # 0.008968609865470852\n",
            "NP* -> PP NP_NNS # 0.002242152466367713\n",
            "NP* -> NP ADVP_RB # 0.002242152466367713\n",
            "NP* -> PP ADVP_RB # 0.002242152466367713\n",
            "NP* -> PP ADJP # 0.002242152466367713\n",
            "NP* -> NNS QP # 0.002242152466367713\n",
            "NP* -> JJS NNS # 0.002242152466367713\n",
            "NP* -> JJ NNS # 0.002242152466367713\n",
            "NP* -> DT NP* # 0.002242152466367713\n",
            "NP* -> SYM NP* # 0.006726457399103139\n",
            "PUNC -> PUNC_t # 1.0\n",
            "TOP -> S_VP PUNC # 0.208955223880597\n",
            "TOP -> SQ PUNC # 0.04477611940298507\n",
            "TOP -> S PUNC # 0.11513859275053305\n",
            "TOP -> SBARQ PUNC # 0.21748400852878466\n",
            "TOP -> FRAG PUNC # 0.05970149253731343\n",
            "TOP -> FRAG_NP PUNC # 0.22388059701492538\n",
            "TOP -> INTJ_UH PUNC # 0.0042643923240938165\n",
            "TOP -> FRAG_NP_NN PUNC # 0.0042643923240938165\n",
            "TOP -> X_SBARQ PUNC # 0.0021321961620469083\n",
            "TOP -> SBAR PUNC # 0.0021321961620469083\n",
            "TOP -> FRAG_VP PUNC # 0.017057569296375266\n",
            "TOP -> NP PUNC # 0.006396588486140725\n",
            "TOP -> FRAG_NP_NNP PUNC # 0.0021321961620469083\n",
            "TOP -> FRAG_WHNP PUNC # 0.07036247334754797\n",
            "TOP -> FRAG_PP PUNC # 0.010660980810234541\n",
            "TOP -> FRAG_ADJP_JJ PUNC # 0.0042643923240938165\n",
            "TOP -> ADJP_JJ PUNC # 0.0021321961620469083\n",
            "TOP -> FRAG_ADJP_JJS PUNC # 0.0021321961620469083\n",
            "TOP -> X_S_VP PUNC # 0.0021321961620469083\n",
            "VBZ -> VBZ_t # 1.0\n",
            "NN -> NN_t # 1.0\n",
            "NP_NN -> NN_t # 1.0\n",
            "VP -> VB NP_NN # 0.044585987261146494\n",
            "VP -> VBG PP # 0.09554140127388536\n",
            "VP -> VBG NP # 0.03821656050955414\n",
            "VP -> VBP NP # 0.10828025477707007\n",
            "VP -> VB S # 0.012738853503184714\n",
            "VP -> VBP S_VP # 0.050955414012738856\n",
            "VP -> VB VP* # 0.12738853503184713\n",
            "VP -> VB S_VP # 0.08280254777070063\n",
            "VP -> VBZ PP # 0.012738853503184714\n",
            "VP -> MD VP # 0.12101910828025478\n",
            "VP -> VB PP # 0.044585987261146494\n",
            "VP -> VB NP # 0.09554140127388536\n",
            "VP -> VP VP* # 0.01910828025477707\n",
            "VP -> TO VP # 0.006369426751592357\n",
            "VP -> VBG NP_NNP # 0.006369426751592357\n",
            "VP -> VBP PP # 0.03184713375796178\n",
            "VP -> VBN PP # 0.01910828025477707\n",
            "VP -> VBP ADJP # 0.006369426751592357\n",
            "VP -> VB NP_NNP # 0.012738853503184714\n",
            "VP -> VBD NP # 0.006369426751592357\n",
            "VP -> VBP FRAG # 0.006369426751592357\n",
            "VP -> VBP VP* # 0.012738853503184714\n",
            "VP -> VB NP_NNS # 0.006369426751592357\n",
            "VP -> VBG VP* # 0.025477707006369428\n",
            "VP -> VBZ NP # 0.006369426751592357\n",
            "SQ* -> NP VP # 0.1323529411764706\n",
            "SQ* -> NP VP_VB # 0.058823529411764705\n",
            "SQ* -> NP_PRP VP # 0.08823529411764706\n",
            "SQ* -> NP_NN VP # 0.014705882352941176\n",
            "SQ* -> NP PP # 0.014705882352941176\n",
            "SQ* -> NP_NP_EX NP # 0.029411764705882353\n",
            "SQ* -> PP PP # 0.25\n",
            "SQ* -> NP SQ* # 0.029411764705882353\n",
            "SQ* -> NP_NP_EX SQ* # 0.25\n",
            "SQ* -> NP ADJP_JJ # 0.014705882352941176\n",
            "SQ* -> VBZ SQ* # 0.014705882352941176\n",
            "SQ* -> NP NP # 0.029411764705882353\n",
            "SQ* -> NP_DT NP # 0.014705882352941176\n",
            "SQ* -> PP SQ* # 0.029411764705882353\n",
            "SQ* -> NP_PRP VP_VB # 0.014705882352941176\n",
            "SQ* -> MD SQ* # 0.014705882352941176\n",
            "SQ -> VBZ SQ* # 0.17333333333333334\n",
            "SQ -> MD SQ* # 0.05333333333333334\n",
            "SQ -> VBP NP # 0.06666666666666667\n",
            "SQ -> VBP SQ* # 0.3466666666666667\n",
            "SQ -> VBZ NP_PRP # 0.02666666666666667\n",
            "SQ -> VBZ NP # 0.29333333333333333\n",
            "SQ -> X SQ* # 0.013333333333333334\n",
            "SQ -> VBP NP_NP_EX # 0.013333333333333334\n",
            "SQ -> INTJ_UH SQ* # 0.013333333333333334\n",
            "NP_PRP -> PRP_t # 1.0\n",
            "VBG -> VBG_t # 1.0\n",
            "S -> NP_PRP VP # 0.5901639344262295\n",
            "S -> NP_NN VP_VBN # 0.01639344262295082\n",
            "S -> NP VP # 0.13114754098360656\n",
            "S -> S S* # 0.03278688524590164\n",
            "S -> INTJ_UH VP # 0.06557377049180328\n",
            "S -> INTJ_UH S* # 0.01639344262295082\n",
            "S -> NP_NNP VP_VBZ # 0.01639344262295082\n",
            "S -> PP S* # 0.03278688524590164\n",
            "S -> NP_PRP S* # 0.01639344262295082\n",
            "S -> ADVP_RB S* # 0.03278688524590164\n",
            "S -> ADVP_RB VP # 0.03278688524590164\n",
            "S -> NP_NN VP # 0.01639344262295082\n",
            "VP_VBN -> VBN_t # 1.0\n",
            "CD -> CD_t # 1.0\n",
            "MD -> MD_t # 1.0\n",
            "ADVP_RB -> RB_t # 1.0\n",
            "NNP -> NNP_t # 1.0\n",
            "ADVP -> ADVP_RB PP # 0.2857142857142857\n",
            "ADVP -> RB PP # 0.14285714285714285\n",
            "ADVP -> NP RB # 0.5714285714285714\n",
            "VP* -> PP ADVP # 0.0064516129032258064\n",
            "VP* -> NP_PRP VP* # 0.01935483870967742\n",
            "VP* -> NP_PRP ADVP # 0.0064516129032258064\n",
            "VP* -> PP NP_NN # 0.0064516129032258064\n",
            "VP* -> PP PP # 0.21935483870967742\n",
            "VP* -> CC VP # 0.04516129032258064\n",
            "VP* -> PRT_RP NP # 0.0064516129032258064\n",
            "VP* -> NP_PRP NP # 0.4967741935483871\n",
            "VP* -> NP_PRP PP # 0.0064516129032258064\n",
            "VP* -> X_TO PP # 0.0064516129032258064\n",
            "VP* -> PP VP* # 0.07096774193548387\n",
            "VP* -> ADVP NP # 0.0064516129032258064\n",
            "VP* -> PP ADJP # 0.0064516129032258064\n",
            "VP* -> ADVP_RB VP* # 0.0064516129032258064\n",
            "VP* -> PP NP # 0.012903225806451613\n",
            "VP* -> NP INTJ_UH # 0.012903225806451613\n",
            "VP* -> NP_NNP PP # 0.03225806451612903\n",
            "VP* -> NP_NNP VP* # 0.012903225806451613\n",
            "VP* -> NP PP # 0.012903225806451613\n",
            "VP* -> RB VP # 0.0064516129032258064\n",
            "NP_CD -> CD_t # 1.0\n",
            "CC -> CC_t # 1.0\n",
            "JJ -> JJ_t # 1.0\n",
            "S* -> CC S # 0.25\n",
            "S* -> NP_PRP VP # 0.625\n",
            "S* -> ADVP_RB VP # 0.125\n",
            "RB -> RB_t # 1.0\n",
            "QP -> RB CD # 0.21428571428571427\n",
            "QP -> IN QP* # 0.14285714285714285\n",
            "QP -> RBR QP* # 0.2857142857142857\n",
            "QP -> CD QP* # 0.21428571428571427\n",
            "QP -> CC JJR # 0.07142857142857142\n",
            "QP -> RB QP* # 0.07142857142857142\n",
            "WRB -> WRB_t # 1.0\n",
            "WHNP -> WRB RB # 0.054945054945054944\n",
            "WHNP -> WHNP ADJP_JJ # 0.01098901098901099\n",
            "WHNP -> WHNP_WDT PP # 0.18681318681318682\n",
            "WHNP -> WDT NN # 0.03296703296703297\n",
            "WHNP -> WDT NNS # 0.6703296703296703\n",
            "WHNP -> WHNP PP # 0.03296703296703297\n",
            "WHNP -> WHADJP NNS # 0.01098901098901099\n",
            "ADJP_JJ -> JJ_t # 1.0\n",
            "VP_VB -> VB_t # 1.0\n",
            "SBARQ -> WHNP SQ # 0.0784313725490196\n",
            "SBARQ -> PP SBARQ* # 0.00980392156862745\n",
            "SBARQ -> WHNP_WP SQ # 0.19607843137254902\n",
            "SBARQ -> WHNP SQ_VP # 0.4117647058823529\n",
            "SBARQ -> WHNP_WHNP SQ # 0.18627450980392157\n",
            "SBARQ -> WHNP_WDT SQ # 0.049019607843137254\n",
            "SBARQ -> WHADVP_WRB SQ # 0.00980392156862745\n",
            "SBARQ -> WHNP_WDT SQ_VP # 0.058823529411764705\n",
            "PRT_RP -> RP_t # 1.0\n",
            "PDT -> PDT_t # 1.0\n",
            "WDT -> WDT_t # 1.0\n",
            "WHNP_WHNP -> WDT NNS # 0.9\n",
            "WHNP_WHNP -> WHADJP NNS # 0.1\n",
            "SQ_VP -> VBP PP # 0.14\n",
            "SQ_VP -> VP VP* # 0.04\n",
            "SQ_VP -> VBP VP # 0.02\n",
            "SQ_VP -> VBP NP # 0.08\n",
            "SQ_VP -> VBZ NP_NN # 0.02\n",
            "SQ_VP -> VBP NP_NN # 0.08\n",
            "SQ_VP -> VBP ADVP_RB # 0.02\n",
            "SQ_VP -> VBZ ADJP_JJ # 0.06\n",
            "SQ_VP -> VBP ADVP_RBS # 0.02\n",
            "SQ_VP -> VBZ ADVP_RBS # 0.02\n",
            "SQ_VP -> VBP VP* # 0.36\n",
            "SQ_VP -> VBP PP_IN # 0.02\n",
            "SQ_VP -> VBP ADJP_JJ # 0.02\n",
            "SQ_VP -> VBP NP_NNS # 0.02\n",
            "SQ_VP -> VBZ NP # 0.02\n",
            "SQ_VP -> VBP NP_NNP # 0.02\n",
            "SQ_VP -> VBZ ADJP_JJR # 0.04\n",
            "SBARQ* -> WHNP_WHNP SQ_VP # 1.0\n",
            "WHNP_WP -> WP_t # 1.0\n",
            "NP_DT -> DT_t # 1.0\n",
            "WP -> WP_t # 1.0\n",
            "X -> WP IN # 0.6666666666666666\n",
            "X -> WRB IN # 0.16666666666666666\n",
            "X -> VBZ NP_DT # 0.16666666666666666\n",
            "FRAG -> X PP # 0.06896551724137931\n",
            "FRAG -> NP INTJ_UH # 0.034482758620689655\n",
            "FRAG -> X NP # 0.06896551724137931\n",
            "FRAG -> NP FRAG* # 0.06896551724137931\n",
            "FRAG -> NP_NNP FRAG* # 0.10344827586206896\n",
            "FRAG -> PP VP # 0.034482758620689655\n",
            "FRAG -> ADVP_RB NP_NNP # 0.034482758620689655\n",
            "FRAG -> NP_NNP PP # 0.1724137931034483\n",
            "FRAG -> PP PP # 0.13793103448275862\n",
            "FRAG -> PP FRAG* # 0.034482758620689655\n",
            "FRAG -> WHNP PP # 0.034482758620689655\n",
            "FRAG -> WHNP FRAG* # 0.06896551724137931\n",
            "FRAG -> NP_NN FRAG* # 0.034482758620689655\n",
            "FRAG -> X NP_NNP # 0.034482758620689655\n",
            "FRAG -> NP NP # 0.06896551724137931\n",
            "VBN -> VBN_t # 1.0\n",
            "ADJP -> JJ PP # 0.2\n",
            "ADJP -> RB ADJP* # 0.2\n",
            "ADJP -> RBS JJ # 0.2\n",
            "ADJP -> JJR PP # 0.4\n",
            "NX_NN -> NN_t # 1.0\n",
            "NX -> NN NN # 0.5\n",
            "NX -> NX_NN NX* # 0.5\n",
            "NX* -> CC NX # 1.0\n",
            "X_TO -> TO_t # 1.0\n",
            "WHADVP_WRB -> WRB_t # 1.0\n",
            "INTJ_UH -> UH_t # 1.0\n",
            "FRAG_NP -> NP PP # 0.05714285714285714\n",
            "FRAG_NP -> NP NP* # 0.21904761904761905\n",
            "FRAG_NP -> NP_NNS NP* # 0.3333333333333333\n",
            "FRAG_NP -> JJS NN # 0.0380952380952381\n",
            "FRAG_NP -> NP VP_VBG # 0.009523809523809525\n",
            "FRAG_NP -> NNP NN # 0.01904761904761905\n",
            "FRAG_NP -> NP_NNS PP # 0.06666666666666667\n",
            "FRAG_NP -> NN NNS # 0.009523809523809525\n",
            "FRAG_NP -> NP_NN PP # 0.05714285714285714\n",
            "FRAG_NP -> NP_NN NP* # 0.0380952380952381\n",
            "FRAG_NP -> JJ NP* # 0.009523809523809525\n",
            "FRAG_NP -> DT NNS # 0.009523809523809525\n",
            "FRAG_NP -> NN NP* # 0.0380952380952381\n",
            "FRAG_NP -> CD NNS # 0.009523809523809525\n",
            "FRAG_NP -> JJ NNS # 0.009523809523809525\n",
            "FRAG_NP -> NP_NNP NP* # 0.009523809523809525\n",
            "FRAG_NP -> JJS NP* # 0.01904761904761905\n",
            "FRAG_NP -> NN NN # 0.02857142857142857\n",
            "FRAG_NP -> NP_NNS ADJP # 0.009523809523809525\n",
            "FRAG_NP -> JJS NNS # 0.009523809523809525\n",
            "JJS -> JJS_t # 1.0\n",
            "NP_NP_EX -> EX_t # 1.0\n",
            "POS -> POS_t # 1.0\n",
            "VP_VBZ -> VBZ_t # 1.0\n",
            "VBD -> VBD_t # 1.0\n",
            "ADJP* -> RB PP # 1.0\n",
            "FRAG* -> NP NP # 0.08333333333333333\n",
            "FRAG* -> NP FRAG* # 0.16666666666666666\n",
            "FRAG* -> PP PP # 0.4166666666666667\n",
            "FRAG* -> PP VP # 0.08333333333333333\n",
            "FRAG* -> PP NP # 0.08333333333333333\n",
            "FRAG* -> PP FRAG* # 0.08333333333333333\n",
            "FRAG* -> PP ADVP_RB # 0.08333333333333333\n",
            "FRAG_NP_NN -> NN_t # 1.0\n",
            "ADVP_RBS -> RBS_t # 1.0\n",
            "WHADJP -> WRB JJ # 1.0\n",
            "PP_IN -> IN_t # 1.0\n",
            "X_SBARQ -> WHNP SQ_VP # 1.0\n",
            "SYM -> SYM_t # 1.0\n",
            "QP* -> JJS CD # 0.14285714285714285\n",
            "QP* -> IN CD # 0.2857142857142857\n",
            "QP* -> CD CD # 0.2857142857142857\n",
            "QP* -> CD QP* # 0.14285714285714285\n",
            "QP* -> IN QP* # 0.07142857142857142\n",
            "QP* -> JJR QP* # 0.07142857142857142\n",
            "X_JJ -> JJ_t # 1.0\n",
            "NP_NNS -> NNS_t # 1.0\n",
            "FRAG_VP -> VBG VP* # 0.75\n",
            "FRAG_VP -> VBG PP # 0.125\n",
            "FRAG_VP -> VBG NP # 0.125\n",
            "VP_VBG -> VBG_t # 1.0\n",
            "PP* -> IN NP # 0.25\n",
            "PP* -> CC PP* # 0.25\n",
            "PP* -> CC PP # 0.25\n",
            "PP* -> IN S_VP_VBG # 0.25\n",
            "SBAR_S_VP -> TO VP_VB # 1.0\n",
            "RBR -> RBR_t # 1.0\n",
            "FRAG_NP_NNP -> NNP_t # 1.0\n",
            "WHNP* -> PP PP # 0.5614035087719298\n",
            "WHNP* -> PP WHNP* # 0.43859649122807015\n",
            "FRAG_WHNP -> WHNP WHNP* # 0.9393939393939394\n",
            "FRAG_WHNP -> NP WHNP* # 0.030303030303030304\n",
            "FRAG_WHNP -> WHNP PP # 0.030303030303030304\n",
            "FRAG_PP -> IN NP_NNP # 0.2\n",
            "FRAG_PP -> IN NP # 0.8\n",
            "S_VP_VBG -> VBG_t # 1.0\n",
            "ADJP_JJR -> JJR_t # 1.0\n",
            "FRAG_ADJP_JJ -> JJ_t # 1.0\n",
            "RBS -> RBS_t # 1.0\n",
            "NP_NNPS -> NNPS_t # 1.0\n",
            "FRAG_ADJP_JJS -> JJS_t # 1.0\n",
            "JJR -> JJR_t # 1.0\n",
            "X_S_VP -> VB VP* # 1.0\n",
            "Number of unique rules: 419\n",
            "\n",
            "Top 10 Most Frequent Rules:\n",
            "IN -> IN_t # Count: 482\n",
            "PUNC -> PUNC_t # Count: 469\n",
            "NP_NNP -> NNP_t # Count: 451\n",
            "NNP -> NNP_t # Count: 408\n",
            "NN -> NN_t # Count: 281\n",
            "TO -> TO_t # Count: 241\n",
            "PP -> IN NP_NNP # Count: 239\n",
            "NNS -> NNS_t # Count: 209\n",
            "DT -> DT_t # Count: 198\n",
            "PP -> IN NP # Count: 197\n",
            "('NNP', 'NNP') # 0.1927536231884058\n",
            "('NP', 'NP*') # 0.11594202898550725\n",
            "('DT', 'NN') # 0.10144927536231885\n",
            "('DT', 'NNS') # 0.08550724637681159\n",
            "('DT', 'NP*') # 0.08405797101449275\n",
            "('NNP', 'NP*') # 0.0463768115942029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: CKY Parsing\n",
        "In this part, you will implement the CKY parsing algorithm."
      ],
      "metadata": {
        "id": "rwt8mwdwNm62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[4 points]** Implement a CKY parser that takes your grammar and a set of POS tags as input, and outputs the highest-probability parse. If you can't find any parse, output a blank line. Use **log-probabilities** to avoid underflow."
      ],
      "metadata": {
        "id": "7PHqLIdiNuVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_cky():\n",
        "  #Grammar and Probabilites\n",
        "    #cfg / counts / probs: representations of your PCFG\n",
        "  rule_counts, prob_rules, cfg_rules = count_rules()\n",
        "\n",
        "  #true POS tags or POS tags as the output of a trained BiLSTM POS tagger\n",
        "  train = read_pos_file(\"/content/nlp_hw2/data/train.pos\")\n",
        "\n",
        "  tag_vocab = set()\n",
        "  for sentence in train:\n",
        "    for item in sentence:\n",
        "      tag_vocab.add(item[1])\n",
        "  tag_vocab.add(\"<UNK>\")\n",
        "\n",
        "  #POS Tagging + diagonal initialization\n",
        "  model = BiLSTMTagger(train, embedding_dim=128, hidden_dim=256)\n",
        "  model.fit(train, lr=0.01, epochs=5)\n",
        "\n",
        "  test = read_pos_file(\"/content/nlp_hw2/data/test.pos\")\n",
        "\n",
        "  for sent_num, sent in enumerate(test[:10]):\n",
        "    words = [word.lower() for word, _ in sent]\n",
        "\n",
        "    n = len(words)\n",
        "    if n == 0:\n",
        "      print()\n",
        "      continue\n",
        "\n",
        "    print(words)\n",
        "    #predicted_tags = model.predict(words) #get predicted values\n",
        "    predicted_tags = [t if t in tag_vocab else \"\" for w, t in sent]\n",
        "    #print(predicted_tags)\n",
        "    #predicted_tags = assign_tags(words, model)\n",
        "    #predicted_tags = [t if t in tag_vocab else \"<UNK>\" for w, t in sent]\n",
        "    #print(predicted_tags)\n",
        "\n",
        "\n",
        "    chart = collections.defaultdict(dict)\n",
        "    backptr = collections.defaultdict(dict)\n",
        "\n",
        "    #Diagonal initialization (length-1 spans [i, i+1)):\n",
        "    for i,tag in enumerate(predicted_tags):\n",
        "      terminal_tag = tag + \"t\"\n",
        "      chart[(i, i+1)][terminal_tag] = 0.0\n",
        "      backptr[(i, i+1)][terminal_tag] = words[i]\n",
        "\n",
        "    #CKY dynsmaic program over spans (big nested loop)\n",
        "    for span_length in range(2, n+1):\n",
        "      for i in range(n-span_length+1):\n",
        "        k = i + span_length\n",
        "        for j in range(i+1, k):\n",
        "\n",
        "          for left_label in chart[(i, j)]:\n",
        "            for right_label in chart[(j, k)]:\n",
        "              # Check if any rule A -> left_label right_label exists in your PCFG\n",
        "              rhs = (left_label, right_label) #<-- may need to come back and check\n",
        "              if rhs in cfg_rules:\n",
        "                for A in cfg_rules[left_label, right_label]:\n",
        "                  candidate_score = chart[(i, j)][left_label] + chart[(j, k)][right_label] + math.log(prob_rules[rhs]) #<-- again MAY NEED TO CHECK IF RHS WORKS\n",
        "                  #chart[(i, j)][B] + chart[(j, k)][C] + math.log(prob)\n",
        "\n",
        "                  if A not in chart[(i, k)] or candidate_score > chart[(i, k)][A]:\n",
        "                    chart[(i, k)][A] = candidate_score\n",
        "                    backptr[(i, k)][A] = (left_label, j, right_label)\n",
        "\n",
        "    #3) ROOT SELECTION, RECONSTRUCTION, PRINTING\n",
        "    def reconstruct(label, i, k):\n",
        "      bp = backptr[(i, k)][label]\n",
        "      if isinstance(bp, str):\n",
        "        return f\"({label} {bp})\"\n",
        "      else:\n",
        "        left_label, j, right_label = bp\n",
        "        left_subtree = reconstruct(left_label, i, j)\n",
        "        right_subtree = reconstruct(right_label, j, k)\n",
        "        return f\"({label} {left_subtree} {right_subtree})\"\n",
        "\n",
        "    if 'TOP' in chart[(0, n)]:\n",
        "      tree = reconstruct('TOP', 0, n)\n",
        "      print(tree)\n",
        "      print(f\"{chart[(0, n)]['TOP']:.4f}\")\n",
        "    else:\n",
        "      print()\n",
        "\n",
        "\n",
        "  #Root selection + backpointer reconstruction +printing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  Goal: Highest-probability parse using a PCFG, with POS tags (from your BiLSTM)\n",
        "        as the terminal layer.\n",
        "\n",
        "  Inputs:\n",
        "    - cfg / counts / probs: representations of your PCFG\n",
        "    - true POS tags or POS tags as the output of a trained BiLSTM POS tagger\n",
        "\n",
        "  Output:\n",
        "    - For each sentence line from stdin or a file, print one bracketed tree to stdout\n",
        "      (or an empty line if no parse) with its probability.\n",
        "\n",
        "  Keep these three phases conceptually separate:\n",
        "    (1) POS tagging + diagonal initialization\n",
        "    (2) CKY dynamic program over spans (big nested loop)\n",
        "    (3) Root selection + backpointer reconstruction + printing\n",
        "\n",
        "  You are free to choose your exact data structures, as long as you can:\n",
        "    - store best scores for labels over spans\n",
        "    - remember how each best item was built (backpointers)\n",
        "    - reconstruct a bracketed tree string at the end\n",
        "\n",
        "  -----------------------------------------------------------------------------\n",
        "  0) GRAMMAR + PROBABILITIES + <unk>\n",
        "  -----------------------------------------------------------------------------\n",
        "  * When reading a sequence of POS tags, map each tag to itself if in vocab,\n",
        "    or else to a special token like \"<unk>\" (but keep original for printing).\n",
        "  * After reconstruction, print the original word as the leaf instead.\n",
        "  * Use log probabilities to avoid underflow:\n",
        "    score = log P(A -> B C) + score(left) + score(right)\n",
        "\n",
        "  -----------------------------------------------------------------------------\n",
        "  1) READ ONE SENTENCE LINE → TOKENS → POS TAGS → DIAGONAL INIT\n",
        "  -----------------------------------------------------------------------------\n",
        "  - Read lines from train.pos as (word, tag) tuples using read_pos_files().\n",
        "  - Run your BiLSTM POS tagger on the words to get one POS per token.\n",
        "    * Hint: to confirm that CKY works, first just use the true POS tags rather than running it through your tagger.\n",
        "\n",
        "  - Create two core tables for CKY (choose your own structures, examples below):\n",
        "      chart:   stores best scores for labels over spans\n",
        "              indexable by span (i, k) and then by label\n",
        "      backptr: stores how that best label@span was formed\n",
        "              (for terminals: the terminal tag; for binary: (left_label, split_index, right_label))\n",
        "\n",
        "    Example shapes (you can pick others):\n",
        "      chart[(i, k)][label]  -> best_score (log-prob or prob)\n",
        "      backptr[(i, k)][label] -> for terminal: stored tag\n",
        "                                for binary: (left_label, j, right_label)\n",
        "\n",
        "  - Diagonal initialization (length-1 spans [i, i+1)):\n",
        "      For each position i:\n",
        "        * Use the POS tag(s) for token i as candidate preterminals.\n",
        "        * Record best scores per POS at (i, i+1).\n",
        "        * Record backptr so reconstruction can print \"(POS word)\".\n",
        "\n",
        "  -----------------------------------------------------------------------------\n",
        "  2) CKY DYNAMIC PROGRAM (THE BIG NESTED LOOP)\n",
        "  -----------------------------------------------------------------------------\n",
        "  The standard CKY fill uses three nested loops over span length, start index,\n",
        "  and split point. Conceptually:\n",
        "\n",
        "    for span_length in 2..n:\n",
        "      for i in 0..(n - span_length):\n",
        "        k = i + span_length\n",
        "        initialize chart[(i, k)] and backptr[(i, k)] (empty)\n",
        "\n",
        "        for j in (i+1)..(k-1):   # split index\n",
        "          # Consider all ways to combine a left piece (i, j) with a right piece (j, k)\n",
        "          for each left_label in chart[(i, j)]:\n",
        "            for each right_label in chart[(j, k)]:\n",
        "              # Check if any rule A -> left_label right_label exists in your PCFG\n",
        "              for each A with P(A -> left_label right_label):\n",
        "                candidate_score = chart[(i, j)][left_label] + \\\n",
        "                                  chart[(j, k)][right_label] + \\\n",
        "                                  log P(A -> left_label right_label)\n",
        "                if candidate_score is better than current chart[(i, k)][A]:\n",
        "                    update chart[(i, k)][A] = candidate_score\n",
        "                    set backptr[(i, k)][A] = (left_label, j, right_label)\n",
        "\n",
        "  Notes:\n",
        "    - Only binary rules are considered here (CNF).\n",
        "    - Keep everything in log-space abd use addition rather than multiplication.\n",
        "\n",
        "  -----------------------------------------------------------------------------\n",
        "  3) ROOT SELECTION, RECONSTRUCTION, PRINTING\n",
        "  -----------------------------------------------------------------------------\n",
        "  - After the table is filled, focus on the full span (0, n).\n",
        "    * Prefer the designated start symbol (e.g., 'TOP') if present at (0, n).\n",
        "    * If 'TOP' is not present, produce an empty parse.\n",
        "\n",
        "  - Reconstruct the tree via backpointers:\n",
        "    * Define a recursive function:\n",
        "        reconstruct(label, i, k):\n",
        "          bp = backptr[(i, k)][label]\n",
        "          if bp is a terminal word (or \"<unk>\"):\n",
        "              return \"(label word_or_original)\"\n",
        "          else:\n",
        "              (left_label, j, right_label) = bp\n",
        "              left_subtree  = reconstruct(left_label,  i, j)\n",
        "              right_subtree = reconstruct(right_label, j, k)\n",
        "              return f\"(label {left_subtree} {right_subtree})\"\n",
        "\n",
        "    * Ensure terminals print the original word here rather than POS tags.\n",
        "\n",
        "  - Output:\n",
        "    * Print the bracketed tree string for each input sentence (or an empty line\n",
        "      if no parse), one sentence per line.\n",
        "    * Print the final score for TOP at (0, n) as a log-prob.\n",
        "\n",
        "  -----------------------------------------------------------------------------\n",
        "  4) PRACTICAL TIPS / DECISIONS (YOU CHOOSE)\n",
        "  -----------------------------------------------------------------------------\n",
        "  - Data structures:\n",
        "      * dict-of-dicts is fine; you can also use defaultdicts.\n",
        "      * You can index chart/backptr by tuples (i, k) or use a 2D list.\n",
        "\n",
        "  - Efficiency:\n",
        "      * Iterate only over labels that actually occur in the subspans.\n",
        "      * If your PCFG is stored by RHS (B, C) -> {A: prob}, you can quickly find\n",
        "        candidate parents A for a given pair (B, C).\n",
        "\n",
        "  - Scores:\n",
        "      * Prefer log-space: add logs instead of multiplying probabilities.\n",
        "\n",
        "  - Debugging:\n",
        "      * Print the diagonal cells after initialization to verify POS entries.\n",
        "      * For a tiny sentence (2–3 words), print intermediate chart cells per length.\n",
        "      * If reconstruction fails, check that backptr entries are actually written\n",
        "        whenever you write a score.\n",
        "\n",
        "  -----------------------------------------------------------------------------\n",
        "  5) MINIMUM I/O LOOP\n",
        "  -----------------------------------------------------------------------------\n",
        "  for each line from stdin:\n",
        "    tokens = line.split()\n",
        "    tags = run_pos_tagger(orig_tokens)\n",
        "\n",
        "    initialize empty chart/backptr\n",
        "\n",
        "    # diagonal init\n",
        "    for i in range(n):\n",
        "      fill chart[(i, i+1)] and backptr[(i, i+1)] with POS entries\n",
        "\n",
        "    # CKY nested loops (length, start i, split j) using binary PCFG rules\n",
        "    fill chart/backptr for spans of length >= 2\n",
        "\n",
        "    if TOP in chart[(0, n)]:\n",
        "        tree_str = reconstruct('TOP', 0, n)   # bracketed\n",
        "        print(tree_str)\n",
        "        print(logprob_of_TOP_to_stderr)\n",
        "    else:\n",
        "        print(\"\")  # empty line if no parse\n",
        "  \"\"\"\n",
        "  #raise NotImplementedError()"
      ],
      "metadata": {
        "id": "_5xDOfWuNz5k"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[1 point]** Test the CKY parser with the test set with **gold POS tags**. **Print** the parse trees for the **first 10 test sentences** (use empty line for no parse). For full credit, these should match or be similar to the true parses in test.trees."
      ],
      "metadata": {
        "id": "msTjbIrvO40p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_parser():\n",
        "  # Load grammar counts, probabilities, and cfg rules\n",
        "  rule_counts, prob_rules, cfg_rules = count_rules()\n",
        "\n",
        "  # Load test data with gold tags\n",
        "  train = read_pos_file(\"/content/nlp_hw2/data/train.pos\")\n",
        "  test = read_pos_file(\"/content/nlp_hw2/data/test.pos\")\n",
        "\n",
        "  tag_vocabualry = set()\n",
        "  for sentence in test:\n",
        "    for item in sentence:\n",
        "      tag_vocabualry.add(item[1])\n",
        "  tag_vocabualry.add(\"<UNK>\")\n",
        "\n",
        "\n",
        "  for sent_num, sent in enumerate(test[:10]):\n",
        "    words = [word.lower() for word, _ in sent]\n",
        "    tags = [t if t in tag_vocabualry else \"\" for w, t in sent]\n",
        "    print(words)\n",
        "    print(tags)\n",
        "\n",
        "    n = len(words)\n",
        "    if n == 0:\n",
        "        continue\n",
        "    chart = collections.defaultdict(dict)\n",
        "    backptr = collections.defaultdict(dict)\n",
        "\n",
        "    # Diagonal initialization using gold tags\n",
        "    for i, tag in enumerate(tags): #tag of predicted tags\n",
        "        terminal_tag = tag + \"t\"\n",
        "        chart[(i, i+1)][terminal_tag] = 0.0  # log(1)\n",
        "        backptr[(i, i+1)][terminal_tag] = words[i]\n",
        "\n",
        "    # CKY dynamic programming\n",
        "    for span_length in range(2, n+1):\n",
        "      for i in range(n - span_length + 1):\n",
        "        k = i + span_length\n",
        "        for j in range(i+1, k):\n",
        "          for left_label in chart[(i, j)]:\n",
        "            for right_label in chart[(j, k)]:\n",
        "              rhs = (left_label, right_label)\n",
        "              if rhs in cfg_rules:\n",
        "                for A in cfg_rules[rhs]:\n",
        "                  prob = prob_rules.get((A, rhs), 0.0)\n",
        "                  if prob == 0:\n",
        "                    continue\n",
        "                  candidate_score = (\n",
        "                      chart[(i, j)][left_label] +\n",
        "                      chart[(j, k)][right_label] +\n",
        "                      math.log(prob)\n",
        "                  )\n",
        "                  if A not in chart[(i, k)] or candidate_score > chart[(i, k)][A]:\n",
        "                      chart[(i, k)][A] = candidate_score\n",
        "                      backptr[(i, k)][A] = (left_label, j, right_label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def reconstruct(label, i, k):\n",
        "      bp = backptr[(i, k)][label]\n",
        "      if isinstance(bp, str):\n",
        "        return f\"({label} {bp})\"\n",
        "      else:\n",
        "        left_label, j, right_label = bp\n",
        "        left_subtree = reconstruct(left_label, i, j)\n",
        "        right_subtree = reconstruct(right_label, j, k)\n",
        "        return f\"({label} {left_subtree} {right_subtree})\"\n",
        "\n",
        "    if 'TOP' in chart[(0, n)]:\n",
        "      tree = reconstruct('TOP', 0, n)\n",
        "      print(tree)\n",
        "      #print(f\"{chart[(0, n)]['TOP']:.4f}\")\n",
        "    else:\n",
        "      print()\n",
        "\n",
        "\n",
        "  #raise NotImplementedError()\n",
        "\n",
        "test_parser()\n",
        "print(\"First 10 Lines\")\n",
        "with open(\"/content/nlp_hw2/data/test.trees\", \"r\") as file:\n",
        "  lines = file.readlines()\n",
        "  for line in lines[:10]:\n",
        "    print(line.strip())\n",
        "    #readable format\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VZo8CB5DO_NP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254aa215-3cb9-467c-cf57-3224d1cb5c17"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'flight', 'should', 'arrive', 'at', 'eleven', 'a.m', 'tomorrow', '.']\n",
            "['DT', 'NN', 'MD', 'VB', 'IN', 'CD', 'RB', 'NN', 'PUNC']\n",
            "\n",
            "['i', 'would', 'like', 'to', 'find', 'a', 'flight', 'that', 'goes', 'from', 'la', 'guardia', 'airport', 'to', 'san', 'jose', '.']\n",
            "['PRP', 'MD', 'VB', 'TO', 'VB', 'DT', 'NN', 'WDT', 'VBZ', 'IN', 'NNP', 'NNP', 'NN', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "\n",
            "['show', 'me', 'the', 'flights', 'from', 'newark', 'to', 'los', 'angeles', '.']\n",
            "['VB', 'PRP', 'DT', 'NNS', 'IN', 'NNP', 'TO', 'NNP', 'NNP', 'PUNC']\n",
            "\n",
            "['what', 'airline', 'is', 'this', '?']\n",
            "['WDT', 'NN', 'VBZ', 'DT', 'PUNC']\n",
            "\n",
            "['show', 'me', 'the', 't', 'w', 'a', 'flight', '.']\n",
            "['VB', 'PRP', 'DT', 'NNP', 'NNP', 'NNP', 'NN', 'PUNC']\n",
            "\n",
            "['i', 'would', 'like', 'to', 'travel', 'to', 'westchester', '.']\n",
            "['PRP', 'MD', 'VB', 'TO', 'VB', 'TO', 'NNP', 'PUNC']\n",
            "\n",
            "['list', 'american', 'airlines', 'flights', 'from', 'new', 'york', 'newark', 'to', 'nashville', '.']\n",
            "['VB', 'NNP', 'NNP', 'NNS', 'IN', 'NNP', 'NNP', 'NNP', 'TO', 'NNP', 'PUNC']\n",
            "\n",
            "['thanks', '.']\n",
            "['UH', 'PUNC']\n",
            "\n",
            "['what', 'flights', 'are', 'there', 'from', 'nashville', 'to', 'houston', 'tomorrow', 'evening', 'that', 'serve', 'dinner', '?']\n",
            "['WDT', 'NNS', 'VBP', 'EX', 'IN', 'NNP', 'TO', 'NNP', 'NN', 'NN', 'WDT', 'VBP', 'NN', 'PUNC']\n",
            "\n",
            "['i', \"'d\", 'like', 'to', 'fly', 'next', 'friday', '.']\n",
            "['PRP', 'MD', 'VB', 'TO', 'VB', 'JJ', 'NNP', 'PUNC']\n",
            "\n",
            "First 10 Lines\n",
            "(TOP (S (NP (DT The) (NN flight)) (VP (MD should) (VP (VB arrive) (VP* (PP (IN at) (NP (CD eleven) (RB a.m))) (NP_NN tomorrow))))) (PUNC .))\n",
            "(TOP (S (NP_PRP I) (VP (MD would) (VP (VB like) (S_VP (TO to) (VP (VB find) (NP (NP (DT a) (NN flight)) (SBAR (WHNP_WDT that) (S_VP (VBZ goes) (VP* (PP (IN from) (NP (NNP La) (NP* (NNP Guardia) (NN airport)))) (PP (TO to) (NP (NNP San) (NNP Jose)))))))))))) (PUNC .))\n",
            "(TOP (S_VP (VB Show) (VP* (NP_PRP me) (NP (NP (DT the) (NNS flights)) (NP* (PP (IN from) (NP_NNP Newark)) (PP (TO to) (NP (NNP Los) (NNP Angeles))))))) (PUNC .))\n",
            "(TOP (SBARQ (WHNP (WDT What) (NN airline)) (SQ (VBZ is) (NP_DT this))) (PUNC ?))\n",
            "(TOP (S_VP (VB Show) (VP* (NP_PRP me) (NP (DT the) (NP* (NNP T) (NP* (NNP W) (NP* (NNP A) (NN flight))))))) (PUNC .))\n",
            "(TOP (S (NP_PRP I) (VP (MD would) (VP (VB like) (S_VP (TO to) (VP (VB travel) (PP (TO to) (NP_NNP Westchester))))))) (PUNC .))\n",
            "(TOP (S_VP (VB List) (NP (NP (NNP American) (NP* (NNP Airlines) (NNS flights))) (NP* (PP (IN from) (NP (NP (NNP New) (NNP York)) (NP_NNP Newark))) (PP (TO to) (NP_NNP Nashville))))) (PUNC .))\n",
            "(TOP (INTJ_UH Thanks) (PUNC .))\n",
            "(TOP (SBARQ (WHNP_WHNP (WDT What) (NNS flights)) (SQ (VBP are) (SQ* (NP_NP_EX there) (SQ* (PP (IN from) (NP_NNP Nashville)) (SQ* (PP (TO to) (NP_NNP Houston)) (SQ* (NP (NN tomorrow) (NN evening)) (SBAR (WHNP_WDT that) (S_VP (VBP serve) (NP_NN dinner))))))))) (PUNC ?))\n",
            "(TOP (S (NP_PRP I) (VP (MD 'd) (VP (VB like) (S_VP (TO to) (VP (VB fly) (NP (JJ next) (NNP Friday))))))) (PUNC .))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[2 points]** Integrate your **Bi-LSTM tagger** to assign POS tags to the words of any input sentence before passing it in to your CKY parser."
      ],
      "metadata": {
        "id": "1rxWxJiEPHq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_tags(sentence, model):\n",
        "\n",
        "  print(\"begining\")\n",
        "  predicted = []\n",
        "  print(sentence)\n",
        "\n",
        "  #model = BiLSTMTagger(sentence, embedding_dim=128, hidden_dim=256) <-- has been initialized outside the function\n",
        "  model.fit(sentence, lr=0.01, epochs=5)\n",
        "\n",
        "  #assign POS tags to words in a sentence:\n",
        "  #for word in sentence:\n",
        "\n",
        "  for sent in sentence:\n",
        "    words = [word.lower() for word, tag in sent]\n",
        "    idxs = [model.words.numberize(word.lower()) for word, tag in sent] # prepped like before\n",
        "    num_sequence = torch.tensor (idxs, dtype=torch.long)\n",
        "\n",
        "    eval = model(num_sequence)\n",
        "    pre = model.predict(eval)\n",
        "    predicted.append(pre)\n",
        "\n",
        "  print(predicted)\n",
        "\n",
        "\n",
        "  # Return word-tag pairs\n",
        "  print(\"end\")\n",
        "  #return list(zip(palabras, predicted_tags))\n",
        "\n",
        "\n",
        "  #raise NotImplementedError"
      ],
      "metadata": {
        "id": "MKrdEKBNQUy2"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[1 point]** Test the full pipeline (**input → tagger → CKY**). **Print** the parse trees for the **first 10 test sentences** (use empty line for no parse) using the predicted POS tags."
      ],
      "metadata": {
        "id": "teR_j7R3Qn0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_pipeline():\n",
        "  sent = read_pos_file(\"/content/nlp_hw2/data/test.pos\")\n",
        "  model = BiLSTMTagger(sent, embedding_dim=128, hidden_dim=256)\n",
        "  assign_tags(sent, model)\n",
        "\n",
        "  parse_cky()\n",
        "\n",
        "  #raise NotImplementedError()\n",
        "\n",
        "test_pipeline()"
      ],
      "metadata": {
        "id": "BHxn4_yQQixu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269e0aad-c6ca-4467-d6b2-38a9d6e86cf7"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<utils.Vocab object at 0x7dd92eb27ad0>\n",
            "begining\n",
            "[[('the', 'DT'), ('flight', 'NN'), ('should', 'MD'), ('arrive', 'VB'), ('at', 'IN'), ('eleven', 'CD'), ('a.m', 'RB'), ('tomorrow', 'NN'), ('.', 'PUNC')], [('i', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('find', 'VB'), ('a', 'DT'), ('flight', 'NN'), ('that', 'WDT'), ('goes', 'VBZ'), ('from', 'IN'), ('la', 'NNP'), ('guardia', 'NNP'), ('airport', 'NN'), ('to', 'TO'), ('san', 'NNP'), ('jose', 'NNP'), ('.', 'PUNC')], [('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('newark', 'NNP'), ('to', 'TO'), ('los', 'NNP'), ('angeles', 'NNP'), ('.', 'PUNC')], [('what', 'WDT'), ('airline', 'NN'), ('is', 'VBZ'), ('this', 'DT'), ('?', 'PUNC')], [('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('t', 'NNP'), ('w', 'NNP'), ('a', 'NNP'), ('flight', 'NN'), ('.', 'PUNC')], [('i', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('travel', 'VB'), ('to', 'TO'), ('westchester', 'NNP'), ('.', 'PUNC')], [('list', 'VB'), ('american', 'NNP'), ('airlines', 'NNP'), ('flights', 'NNS'), ('from', 'IN'), ('new', 'NNP'), ('york', 'NNP'), ('newark', 'NNP'), ('to', 'TO'), ('nashville', 'NNP'), ('.', 'PUNC')], [('thanks', 'UH'), ('.', 'PUNC')], [('what', 'WDT'), ('flights', 'NNS'), ('are', 'VBP'), ('there', 'EX'), ('from', 'IN'), ('nashville', 'NNP'), ('to', 'TO'), ('houston', 'NNP'), ('tomorrow', 'NN'), ('evening', 'NN'), ('that', 'WDT'), ('serve', 'VBP'), ('dinner', 'NN'), ('?', 'PUNC')], [('i', 'PRP'), (\"'d\", 'MD'), ('like', 'VB'), ('to', 'TO'), ('fly', 'VB'), ('next', 'JJ'), ('friday', 'NNP'), ('.', 'PUNC')], [('which', 'WDT'), ('of', 'IN'), ('those', 'DT'), ('are', 'VBP'), ('nonstop', 'JJ'), ('?', 'PUNC')], [('i', 'PRP'), ('want', 'VBP'), ('a', 'DT'), ('flight', 'NN'), ('from', 'IN'), ('ontario', 'NNP'), ('to', 'TO'), ('chicago', 'NNP'), ('.', 'PUNC')], [('which', 'WDT'), ('ones', 'NNS'), ('arrive', 'VBP'), ('early', 'RB'), ('in', 'IN'), ('the', 'DT'), ('day', 'NN'), ('?', 'PUNC')], [('which', 'WDT'), ('of', 'IN'), ('these', 'DT'), ('flights', 'NNS'), ('depart', 'VBP'), ('latest', 'RBS'), ('?', 'PUNC')], [('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('baltimore', 'NNP'), ('to', 'TO'), ('oakland', 'NNP'), ('.', 'PUNC')], [('are', 'VBP'), ('there', 'EX'), ('any', 'DT'), ('other', 'JJ'), ('fares', 'NNS'), ('from', 'IN'), ('tacoma', 'NNP'), ('to', 'TO'), ('montreal', 'NNP'), ('?', 'PUNC')], [('which', 'WDT'), ('is', 'VBZ'), ('the', 'DT'), ('latest', 'JJS'), ('?', 'PUNC')], [('what', 'WDT'), ('type', 'NN'), ('of', 'IN'), ('aircraft', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('on', 'IN'), ('those', 'DT'), ('flights', 'NNS'), ('?', 'PUNC')], [('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('lowest', 'JJS'), ('fare', 'NN'), ('for', 'IN'), ('united', 'NNP'), ('airlines', 'NNP'), ('flight', 'NN'), ('four', 'CD'), ('thirty', 'CD'), ('?', 'PUNC')], [('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('salt', 'NNP'), ('lake', 'NNP'), ('city', 'NNP'), ('to', 'TO'), ('phoenix', 'NNP'), ('.', 'PUNC')], [('show', 'VB'), ('flights', 'NNS'), ('.', 'PUNC')], [('what', 'WP'), ('is', 'VBZ'), ('b', 'SYM'), ('n', 'SYM'), ('a', 'SYM'), ('?', 'PUNC')], [('what', 'WDT'), ('flights', 'NNS'), ('are', 'VBP'), ('there', 'EX'), ('from', 'IN'), ('newark', 'NNP'), ('to', 'TO'), ('tampa', 'NNP'), ('?', 'PUNC')], [('what', 'WDT'), ('flights', 'NNS'), ('are', 'VBP'), ('from', 'IN'), ('memphis', 'NNP'), ('to', 'TO'), ('las', 'NNP'), ('vegas', 'NNP'), ('?', 'PUNC')], [('show', 'VB'), ('cost', 'NN'), ('of', 'IN'), ('u', 'NNP'), ('s', 'NNP'), ('air', 'NNP'), ('fifteen', 'CD'), ('twenty', 'CD'), ('three', 'CD'), ('and', 'CC'), ('u', 'NNP'), ('s', 'NNP'), ('air', 'NNP'), ('seven', 'CD'), ('eight', 'CD'), ('one', 'CD'), ('.', 'PUNC')], [('flights', 'NNS'), ('from', 'IN'), ('charlotte', 'NNP'), ('to', 'TO'), ('baltimore', 'NNP'), ('.', 'PUNC')], [('show', 'VB'), ('me', 'PRP'), ('flights', 'NNS'), ('that', 'WDT'), ('leave', 'VBP'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('.', 'PUNC')], [('change', 'VB'), ('newark', 'NNP'), ('city', 'NNP'), ('to', 'TO'), ('new', 'NNP'), ('york', 'NNP'), ('city', 'NNP'), ('.', 'PUNC')], [('show', 'VB'), ('me', 'PRP'), ('flights', 'NNS'), ('from', 'IN'), ('los', 'NNP'), ('angeles', 'NNP'), ('to', 'TO'), ('pittsburgh', 'NNP'), ('on', 'IN'), ('monday', 'NNP'), ('evening', 'NN'), ('.', 'PUNC')], [('show', 'VB'), ('all', 'DT'), ('fares', 'NNS'), ('.', 'PUNC')], [('what', 'WP'), (\"'s\", 'VBZ'), ('the', 'DT'), ('latest', 'JJS'), ('flight', 'NN'), ('from', 'IN'), ('houston', 'NNP'), ('to', 'TO'), ('dallas', 'NNP'), ('?', 'PUNC')], [('from', 'IN'), ('toronto', 'NNP'), ('to', 'TO'), ('atlanta', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('afternoon', 'NN'), ('.', 'PUNC')], [('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('denver', 'NNP'), ('to', 'TO'), ('salt', 'NNP'), ('lake', 'NNP'), ('city', 'NNP'), ('.', 'PUNC')], [('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('atlanta', 'NNP'), ('to', 'TO'), ('washington', 'NNP'), ('.', 'PUNC')], [('what', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('price', 'NN'), ('of', 'IN'), ('flights', 'NNS'), ('from', 'IN'), ('indianapolis', 'NNP'), ('to', 'TO'), ('memphis', 'NNP'), ('?', 'PUNC')], [('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('phoenix', 'NNP'), ('to', 'TO'), ('las', 'NNP'), ('vegas', 'NNP'), ('on', 'IN'), ('saturday', 'NNP'), ('.', 'PUNC')], [('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('detroit', 'NNP'), ('to', 'TO'), ('saint', 'NNP'), ('petersburg', 'NNP'), ('.', 'PUNC')], [('what', 'WDT'), ('flights', 'NNS'), ('from', 'IN'), ('minneapolis', 'NNP'), ('to', 'TO'), ('pittsburgh', 'NNP'), ('.', 'PUNC')], [('flights', 'NNS'), ('from', 'IN'), ('cleveland', 'NNP'), ('to', 'TO'), ('kansas', 'NNP'), ('city', 'NNP'), ('on', 'IN'), ('monday', 'NNP'), ('.', 'PUNC')], [('flights', 'NNS'), ('from', 'IN'), ('kansas', 'NNP'), ('city', 'NNP'), ('to', 'TO'), ('cleveland', 'NNP'), ('.', 'PUNC')], [('flights', 'NNS'), ('from', 'IN'), ('ontario', 'NNP'), ('to', 'TO'), ('orlando', 'NNP'), ('.', 'PUNC')], [('cheapest', 'JJS'), ('flight', 'NN'), ('from', 'IN'), ('indianapolis', 'NNP'), ('to', 'TO'), ('memphis', 'NNP'), ('.', 'PUNC')], [('flights', 'NNS'), ('from', 'IN'), ('pittsburgh', 'NNP'), ('to', 'TO'), ('newark', 'NNP'), ('.', 'PUNC')], [('what', 'WP'), ('does', 'VBZ'), ('that', 'DT'), ('mean', 'NN'), ('?', 'PUNC')], [('round', 'JJ'), ('trip', 'NN'), ('flights', 'NNS'), ('without', 'IN'), ('restriction', 'NN'), ('code', 'NN'), ('a', 'SYM'), ('p', 'SYM'), ('slash', 'SYM'), ('five', 'CD'), ('seven', 'CD'), ('.', 'PUNC')], [('first', 'JJ'), ('class', 'NN'), ('fare', 'NN'), ('.', 'PUNC')], [('first', 'JJ'), ('class', 'NN'), ('.', 'PUNC')], [('return', 'NN'), ('flight', 'NN'), ('.', 'PUNC')], [('departing', 'VBG'), ('after', 'IN'), ('five', 'CD'), ('p.m', 'RB'), ('.', 'PUNC')], [('on', 'IN'), ('wednesdays', 'NNP'), ('after', 'IN'), ('five', 'CD'), ('p.m', 'RB'), ('.', 'PUNC')], [('flights', 'NNS'), ('from', 'IN'), ('westchester', 'NNP'), ('county', 'NNP'), ('to', 'TO'), ('san', 'NNP'), ('francisco', 'NNP'), ('daily', 'RB'), ('.', 'PUNC')], [('newark', 'NNP'), ('to', 'TO'), ('cleveland', 'NNP'), ('.', 'PUNC')], [('flights', 'NNS'), ('on', 'IN'), ('t', 'NNP'), ('w', 'NNP'), ('a', 'NNP'), ('.', 'PUNC')], [('show', 'VB'), ('the', 'DT'), ('fares', 'NNS'), ('.', 'PUNC')], [('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('return', 'NN'), ('trips', 'NNS'), ('.', 'PUNC')], [('show', 'VB'), ('me', 'PRP'), ('all', 'DT'), ('flights', 'NNS'), ('on', 'IN'), ('southwest', 'NNP'), ('airlines', 'NNP'), ('from', 'IN'), ('san', 'NNP'), ('diego', 'NNP'), ('to', 'TO'), ('san', 'NNP'), ('francisco', 'NNP'), ('.', 'PUNC')], [('show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('flights', 'NNS'), ('from', 'IN'), ('san', 'NNP'), ('diego', 'NNP'), ('to', 'TO'), ('washington', 'NNP'), ('d', 'NNP'), ('c', 'NNP'), ('.', 'PUNC')], [('explain', 'VB'), ('the', 'DT'), ('restrictions', 'NNS'), ('.', 'PUNC')]]\n",
            "avg loss per token: 1.229307870271692;\t\truntime: 0.7103297710418701\n",
            "avg loss per token: 0.3459236792405136;\t\truntime: 0.6951947212219238\n",
            "avg loss per token: 0.03882481755436888;\t\truntime: 0.6874904632568359\n",
            "avg loss per token: 0.009509470557218421;\t\truntime: 0.7131154537200928\n",
            "avg loss per token: 0.0015241532487850456;\t\truntime: 0.716590166091919\n",
            "[tensor([11, 12,  1, 20,  8]), tensor([ 5, 13, 10, 13,  5,  1,  2,  8]), tensor([ 4, 13, 13, 10, 13, 13, 13,  8]), tensor([11, 14, 16,  7,  5,  1,  2,  8]), tensor([18,  2,  8]), tensor([11, 14,  5, 13, 10, 13, 13,  5, 13,  8]), tensor([ 4,  9,  1, 14,  5, 13, 10, 13,  8]), tensor([ 9,  3,  4, 10,  4,  1,  2, 11, 12,  5, 13, 13,  2, 10, 13, 13,  8]), tensor([11, 14, 16, 17,  5, 13, 10, 13,  2,  2, 11, 16,  2,  8]), tensor([ 4,  9,  1,  2, 14,  8]), tensor([20,  2,  5, 13, 10, 13,  8]), tensor([18,  2, 14,  5,  2,  2, 23, 23, 23,  6,  6,  8]), tensor([ 4,  1, 14,  8]), tensor([11,  5,  1, 16, 18,  8]), tensor([14,  5, 13, 13, 10, 13, 13,  7,  8]), tensor([25,  5,  6,  7,  8]), tensor([ 4,  9, 14,  5, 13, 13, 10, 13,  5, 13,  2,  8]), tensor([ 4,  2,  5, 13, 13, 13,  6,  6,  6, 24, 13, 13, 13,  6,  6,  6,  8]), tensor([11,  5,  1, 14, 16, 19,  8]), tensor([ 9, 16,  1,  2,  5, 13, 10, 13,  8]), tensor([ 4,  9,  1, 14,  5, 13, 13, 10, 13, 13, 13,  8]), tensor([ 4, 14,  8]), tensor([11,  2,  5,  2, 12, 21,  5,  1, 14,  8]), tensor([22, 12,  1, 20,  2,  5, 13, 13,  2,  6,  6,  8]), tensor([11, 14,  5, 13, 10, 13,  8]), tensor([ 4,  1, 14,  8]), tensor([11,  2, 12,  1,  8]), tensor([11, 14,  5, 13, 10, 13, 13,  8]), tensor([14,  5, 13, 10, 13,  8]), tensor([14,  5, 13, 10, 13,  8]), tensor([ 4, 13, 13, 14,  5, 13, 13, 13, 10, 13,  8]), tensor([22, 12,  1, 20,  2,  5, 13, 10, 13,  8]), tensor([ 9,  3,  4, 10,  4, 10, 13,  8]), tensor([2, 2, 8]), tensor([22, 12,  1,  2,  8]), tensor([11, 14,  5, 13, 10, 13,  8]), tensor([ 4,  9,  1, 13, 13, 13,  2,  8]), tensor([14,  5, 13, 10, 13,  8]), tensor([ 4,  9,  1, 14,  5, 13, 13,  5, 13, 13, 10, 13, 13,  8]), tensor([11, 14, 16, 17,  5, 13, 10, 13,  8]), tensor([14,  5, 13, 10, 13, 13,  5, 13,  8]), tensor([11, 14, 16,  5, 13, 10, 13, 13,  8]), tensor([13, 10, 13,  8]), tensor([11, 14,  5, 13, 10, 13, 13, 13,  8]), tensor([ 4,  9, 14, 11, 16,  5,  1,  2,  8]), tensor([14,  5, 13, 13, 13,  8]), tensor([14,  5, 13, 13, 10, 13,  8]), tensor([ 4,  9,  1, 14,  5, 13, 10, 13, 13,  8]), tensor([ 4,  9,  1, 14,  5, 13, 13, 13, 10, 13,  8]), tensor([22, 12,  1,  2,  5, 14,  5, 13, 10, 13,  8]), tensor([1, 2, 3, 4, 5, 6, 7, 2, 8]), tensor([ 9,  3,  4, 10,  4, 18, 13,  8]), tensor([22, 12, 23, 23, 23,  8]), tensor([16, 17,  1, 18, 14,  5, 13, 10, 13,  8]), tensor([18,  2,  2,  8]), tensor([ 5, 13,  5,  6,  7,  8]), tensor([15,  8]), tensor([ 4,  1, 14,  8])]\n",
            "end\n",
            "<utils.Vocab object at 0x7dd933a863c0>\n",
            "avg loss per token: 0.5220223015045748;\t\truntime: 6.012579441070557\n",
            "avg loss per token: 0.10595774861816956;\t\truntime: 6.4143853187561035\n",
            "avg loss per token: 0.05086390154483286;\t\truntime: 5.766317367553711\n",
            "avg loss per token: 0.03128278427961679;\t\truntime: 6.690797805786133\n",
            "avg loss per token: 0.011576908150181118;\t\truntime: 5.725550889968872\n",
            "['the', 'flight', 'should', 'arrive', 'at', 'eleven', 'a.m', 'tomorrow', '.']\n",
            "\n",
            "['i', 'would', 'like', 'to', 'find', 'a', 'flight', 'that', 'goes', 'from', 'la', 'guardia', 'airport', 'to', 'san', 'jose', '.']\n",
            "\n",
            "['show', 'me', 'the', 'flights', 'from', 'newark', 'to', 'los', 'angeles', '.']\n",
            "\n",
            "['what', 'airline', 'is', 'this', '?']\n",
            "\n",
            "['show', 'me', 'the', 't', 'w', 'a', 'flight', '.']\n",
            "\n",
            "['i', 'would', 'like', 'to', 'travel', 'to', 'westchester', '.']\n",
            "\n",
            "['list', 'american', 'airlines', 'flights', 'from', 'new', 'york', 'newark', 'to', 'nashville', '.']\n",
            "\n",
            "['thanks', '.']\n",
            "\n",
            "['what', 'flights', 'are', 'there', 'from', 'nashville', 'to', 'houston', 'tomorrow', 'evening', 'that', 'serve', 'dinner', '?']\n",
            "\n",
            "['i', \"'d\", 'like', 'to', 'fly', 'next', 'friday', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[2 points]** Free response: For the first 10 sentences of test.pos…\n",
        "- Is there a difference in which sentences it fails to parse given gold tags vs. your tagger's outputs? Why or why not?\n",
        ">  It's seems to do words once the taggers output is added. But there is a high (very likely) possibility that this is a result from the way I implemented it (was having many issues)\n",
        "\n",
        "- Which ones does it do well on (i.e., match the true parse in test.trees), and why?\n",
        "> It seem's to do well overall since the words are pretty common and the sentences themselves aren't complicated in structure and grammatically.\n",
        "- Which ones does it do poorly on (but still produces a parse), and why?\n",
        "> Some of the issues my come from past problems in other functions (this was quite difficult so I wouldn't be surprised if that was the case). But tis seems for me at least, it would make mistaks on similar tags.\n"
      ],
      "metadata": {
        "id": "KzmDfxlxQx1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations! That's a wrap on HW2. Onto neural methods and greater generalizability.**"
      ],
      "metadata": {
        "id": "o0V01er4RDAh"
      }
    }
  ]
}